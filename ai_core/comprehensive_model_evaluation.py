#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæ¨¡å‹è¯„æµ‹è„šæœ¬
å¯¹Qwenå’ŒSmolVLM2æ¨¡å‹è¿›è¡Œå…¨é¢çš„æ€§èƒ½å’Œè´¨é‡è¯„æµ‹
åŒ…æ‹¬å“åº”è´¨é‡ã€æ¨ç†é€Ÿåº¦ã€èµ„æºå ç”¨ã€å‡†ç¡®æ€§ç­‰å¤šä¸ªç»´åº¦
"""

import os
import sys
import json
import time
import psutil
import logging
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from PIL import Image
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥æ¨¡å‹å®¢æˆ·ç«¯
try:
    from models.client.qwen3_client import Qwen3Client
    QWEN3_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Qwen3å®¢æˆ·ç«¯å¯¼å…¥å¤±è´¥: {e}")
    QWEN3_AVAILABLE = False

try:
    from models.client.smolvlm2_client import SmolVLM2Client
    SMOLVLM2_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  SmolVLM2å®¢æˆ·ç«¯å¯¼å…¥å¤±è´¥: {e}")
    SMOLVLM2_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ComprehensiveModelEvaluator:
    """ç»¼åˆæ¨¡å‹è¯„æµ‹å™¨"""
    
    def __init__(self):
        self.qwen3_client = None
        self.smolvlm2_client = None
        
        # è¯„æµ‹ç”¨ä¾‹
        self.test_cases = {
            'basic_qa': [
                "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹é’èŠ±ç“·çš„ç‰¹ç‚¹",
                "æ˜ä»£ç“·å™¨æœ‰å“ªäº›ä¸»è¦ç‰¹å¾ï¼Ÿ",
                "å¦‚ä½•é‰´åˆ«å¤è‘£çš„çœŸä¼ªï¼Ÿ",
                "å®‹ä»£å®˜çª‘ç“·å™¨çš„ä»·å€¼å¦‚ä½•ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯é‡‰é‡Œçº¢å·¥è‰ºï¼Ÿ"
            ],
            'complex_analysis': [
                "è¯·è¯¦ç»†åˆ†æå”ä¸‰å½©çš„åˆ¶ä½œå·¥è‰ºã€å†å²èƒŒæ™¯å’Œæ”¶è—ä»·å€¼ï¼Œå¹¶è¯´æ˜å…¶åœ¨ä¸­å›½é™¶ç“·å²ä¸Šçš„åœ°ä½",
                "æ¯”è¾ƒåˆ†ææ˜ä»£é’èŠ±ç“·å’Œæ¸…ä»£é’èŠ±ç“·åœ¨å·¥è‰ºã€çº¹é¥°ã€èƒè´¨ç­‰æ–¹é¢çš„å·®å¼‚",
                "ä»è€ƒå¤å­¦è§’åº¦åˆ†ææ±çª‘ç“·å™¨çš„å‘ç°æ„ä¹‰å’Œç ”ç©¶ä»·å€¼",
                "è®ºè¿°æ™¯å¾·é•‡åœ¨ä¸­å›½ç“·å™¨å‘å±•å²ä¸Šçš„é‡è¦ä½œç”¨å’Œå†å²åœ°ä½"
            ],
            'professional_terms': [
                "ä»€ä¹ˆæ˜¯'å¼€ç‰‡'ç°è±¡ï¼Ÿå®ƒå¯¹å¤ç“·é‰´å®šæœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ",
                "è¯·è§£é‡Š'èƒéª¨'ã€'é‡‰è‰²'ã€'ç«å€™'è¿™ä¸‰ä¸ªé™¶ç“·æœ¯è¯­",
                "'çª‘å˜'æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæ˜¯å¦‚ä½•å½¢æˆçš„ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯'æ”¯é’‰ç—•'ï¼Ÿå®ƒèƒ½è¯´æ˜ä»€ä¹ˆé—®é¢˜ï¼Ÿ"
            ],
            'reasoning_tasks': [
                "å¦‚æœä¸€ä»¶ç“·å™¨åº•éƒ¨æœ‰'å¤§æ¸…åº·ç†™å¹´åˆ¶'æ¬¾è¯†ï¼Œä½†é‡‰è‰²åç°ä»£ï¼Œèƒè´¨è¾ƒè½»ï¼Œä½ ä¼šå¦‚ä½•åˆ¤æ–­å…¶çœŸä¼ªï¼Ÿè¯·è¯´æ˜ç†ç”±ã€‚",
                "æŸæ”¶è—å®¶å£°ç§°æ‹¥æœ‰ä¸€ä»¶æ±çª‘å¤©é’é‡‰æ´—ï¼Œä½†ä¸“å®¶è´¨ç–‘å…¶çœŸå®æ€§ã€‚è¯·åˆ†æå¯èƒ½çš„é‰´å®šè¦ç‚¹ã€‚",
                "åœ¨æ‹å–ä¼šä¸Šçœ‹åˆ°ä¸€ä»¶æ ‡æ³¨ä¸º'å®‹ä»£å®šçª‘ç™½ç“·'çš„å™¨ç‰©ï¼Œä»·æ ¼å¼‚å¸¸ä¾¿å®œï¼Œå¯èƒ½å­˜åœ¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ"
            ]
        }
        
        # è¯„æµ‹æŒ‡æ ‡
        self.evaluation_metrics = {
            'response_time': [],
            'response_length': [],
            'memory_usage': [],
            'cpu_usage': [],
            'success_rate': 0,
            'quality_scores': []
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            import torch
            gpu_info = {
                'cuda_available': torch.cuda.is_available(),
                'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
                'gpu_names': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())] if torch.cuda.is_available() else []
            }
        except:
            gpu_info = {'cuda_available': False, 'gpu_count': 0, 'gpu_names': []}
        
        return {
            'timestamp': datetime.now().isoformat(),
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total / (1024**3),  # GB
            'python_version': sys.version,
            'platform': sys.platform,
            'gpu_info': gpu_info
        }
    
    def monitor_resources(self, duration: float = 1.0) -> Dict[str, float]:
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        start_time = time.time()
        cpu_samples = []
        memory_samples = []
        
        while time.time() - start_time < duration:
            cpu_samples.append(psutil.cpu_percent())
            memory_samples.append(psutil.virtual_memory().percent)
            time.sleep(0.1)
        
        return {
            'avg_cpu_usage': np.mean(cpu_samples),
            'max_cpu_usage': np.max(cpu_samples),
            'avg_memory_usage': np.mean(memory_samples),
            'max_memory_usage': np.max(memory_samples)
        }
    
    def evaluate_response_quality(self, prompt: str, response: str) -> Dict[str, Any]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        quality_metrics = {
            'length': len(response),
            'word_count': len(response.split()),
            'has_professional_terms': 0,
            'completeness_score': 0,
            'relevance_score': 0,
            'quality_score': 0
        }
        
        # ä¸“ä¸šæœ¯è¯­æ£€æµ‹
        professional_terms = [
            'é’èŠ±', 'é‡‰è‰²', 'èƒè´¨', 'çª‘å£', 'æ¬¾è¯†', 'çº¹é¥°', 'å·¥è‰º', 'æœä»£',
            'ç“·å™¨', 'é™¶å™¨', 'å®˜çª‘', 'æ°‘çª‘', 'å¼€ç‰‡', 'é‡‰é¢', 'èƒéª¨', 'ç«å€™',
            'æ™¯å¾·é•‡', 'æ±çª‘', 'å®šçª‘', 'é’§çª‘', 'å“¥çª‘', 'å”ä¸‰å½©', 'å®‹ç“·'
        ]
        
        term_count = sum(1 for term in professional_terms if term in response)
        quality_metrics['has_professional_terms'] = term_count
        
        # å®Œæ•´æ€§è¯„åˆ†ï¼ˆåŸºäºé•¿åº¦å’Œç»“æ„ï¼‰
        if len(response) > 200:
            quality_metrics['completeness_score'] = min(100, len(response) / 5)
        else:
            quality_metrics['completeness_score'] = len(response) / 2
        
        # ç›¸å…³æ€§è¯„åˆ†ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
        prompt_keywords = set(prompt.lower().split())
        response_keywords = set(response.lower().split())
        relevance = len(prompt_keywords.intersection(response_keywords)) / len(prompt_keywords) * 100
        quality_metrics['relevance_score'] = relevance
        
        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality_score = (
            min(quality_metrics['completeness_score'], 40) +  # å®Œæ•´æ€§ 40%
            min(quality_metrics['relevance_score'], 30) +     # ç›¸å…³æ€§ 30%
            min(term_count * 5, 30)                           # ä¸“ä¸šæ€§ 30%
        )
        quality_metrics['quality_score'] = quality_score
        
        return quality_metrics
    
    def test_model_performance(self, model_client, model_name: str, test_cases: List[str]) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        print(f"\nğŸ” æµ‹è¯•{model_name}æ¨¡å‹æ€§èƒ½...")
        print("="*60)
        
        results = {
            'model_name': model_name,
            'test_results': [],
            'performance_metrics': {
                'total_tests': len(test_cases),
                'successful_tests': 0,
                'failed_tests': 0,
                'avg_response_time': 0,
                'avg_response_length': 0,
                'avg_quality_score': 0,
                'resource_usage': {
                    'avg_cpu_usage': 0,
                    'avg_memory_usage': 0
                }
            }
        }
        
        response_times = []
        response_lengths = []
        quality_scores = []
        cpu_usages = []
        memory_usages = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_cases)}: {test_case[:50]}...")
            
            try:
                # å¼€å§‹èµ„æºç›‘æ§
                monitor_thread = threading.Thread(
                    target=lambda: self._monitor_during_inference(cpu_usages, memory_usages)
                )
                monitor_thread.daemon = True
                monitor_thread.start()
                
                # æ‰§è¡Œæ¨ç†
                start_time = time.time()
                
                if hasattr(model_client, 'text_chat'):
                    response = model_client.text_chat(test_case)
                else:
                    response = "æ¨¡å‹ä¸æ”¯æŒtext_chatæ–¹æ³•"
                
                response_time = time.time() - start_time
                
                # åœæ­¢ç›‘æ§
                monitor_thread.join(timeout=0.1)
                
                # è¯„ä¼°å“åº”è´¨é‡
                quality_metrics = self.evaluate_response_quality(test_case, response)
                
                # è®°å½•ç»“æœ
                test_result = {
                    'test_case': test_case,
                    'response': response,
                    'response_time': response_time,
                    'response_length': len(response),
                    'quality_metrics': quality_metrics,
                    'success': True
                }
                
                results['test_results'].append(test_result)
                results['performance_metrics']['successful_tests'] += 1
                
                # æ”¶é›†ç»Ÿè®¡æ•°æ®
                response_times.append(response_time)
                response_lengths.append(len(response))
                quality_scores.append(quality_metrics['quality_score'])
                
                print(f"âœ… æˆåŠŸ - å“åº”æ—¶é—´: {response_time:.2f}ç§’, è´¨é‡è¯„åˆ†: {quality_metrics['quality_score']:.1f}")
                
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ å¤±è´¥: {error_msg}")
                
                results['test_results'].append({
                    'test_case': test_case,
                    'error': error_msg,
                    'success': False
                })
                results['performance_metrics']['failed_tests'] += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if response_times:
            results['performance_metrics']['avg_response_time'] = np.mean(response_times)
            results['performance_metrics']['avg_response_length'] = np.mean(response_lengths)
            results['performance_metrics']['avg_quality_score'] = np.mean(quality_scores)
        
        if cpu_usages:
            results['performance_metrics']['resource_usage']['avg_cpu_usage'] = np.mean(cpu_usages)
        if memory_usages:
            results['performance_metrics']['resource_usage']['avg_memory_usage'] = np.mean(memory_usages)
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = results['performance_metrics']['successful_tests'] / results['performance_metrics']['total_tests']
        results['performance_metrics']['success_rate'] = success_rate
        
        return results
    
    def _monitor_during_inference(self, cpu_usages: List[float], memory_usages: List[float]):
        """æ¨ç†è¿‡ç¨‹ä¸­ç›‘æ§èµ„æºä½¿ç”¨"""
        start_time = time.time()
        while time.time() - start_time < 30:  # æœ€å¤šç›‘æ§30ç§’
            try:
                cpu_usages.append(psutil.cpu_percent())
                memory_usages.append(psutil.virtual_memory().percent)
                time.sleep(0.5)
            except:
                break
    
    def test_multimodal_performance(self, model_client, test_images: List[str]) -> Dict[str, Any]:
        """æµ‹è¯•å¤šæ¨¡æ€æ€§èƒ½"""
        if not test_images:
            return {'error': 'æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡'}
        
        print(f"\nğŸ–¼ï¸  æµ‹è¯•å¤šæ¨¡æ€æ€§èƒ½ï¼ˆ{len(test_images)}å¼ å›¾ç‰‡ï¼‰...")
        
        results = {
            'multimodal_tests': [],
            'avg_response_time': 0,
            'success_rate': 0
        }
        
        multimodal_prompts = [
            "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„å¤è‘£ï¼Œæè¿°å…¶ç‰¹å¾å’Œå¯èƒ½çš„ä»·å€¼",
            "è¿™ä»¶å¤è‘£å¯èƒ½æ¥è‡ªå“ªä¸ªå†å²æ—¶æœŸï¼Ÿè¯·è¯´æ˜åˆ¤æ–­ä¾æ®",
            "ä»å·¥è‰ºè§’åº¦åˆ†æè¿™ä»¶å¤è‘£çš„åˆ¶ä½œç‰¹ç‚¹"
        ]
        
        response_times = []
        successful_tests = 0
        
        for i, image_path in enumerate(test_images[:3]):  # æœ€å¤šæµ‹è¯•3å¼ å›¾ç‰‡
            try:
                image = Image.open(image_path)
                prompt = multimodal_prompts[i % len(multimodal_prompts)]
                
                print(f"ğŸ“¸ æµ‹è¯•å›¾ç‰‡ {i+1}: {os.path.basename(image_path)}")
                
                start_time = time.time()
                if hasattr(model_client, 'chat_about_antique'):
                    response = model_client.chat_about_antique(image, prompt)
                else:
                    response = "æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€åŠŸèƒ½"
                
                response_time = time.time() - start_time
                response_times.append(response_time)
                successful_tests += 1
                
                results['multimodal_tests'].append({
                    'image_path': image_path,
                    'prompt': prompt,
                    'response': response,
                    'response_time': response_time,
                    'success': True
                })
                
                print(f"âœ… å¤šæ¨¡æ€æµ‹è¯•æˆåŠŸ - å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                
            except Exception as e:
                print(f"âŒ å¤šæ¨¡æ€æµ‹è¯•å¤±è´¥: {e}")
                results['multimodal_tests'].append({
                    'image_path': image_path,
                    'error': str(e),
                    'success': False
                })
        
        if response_times:
            results['avg_response_time'] = np.mean(response_times)
        results['success_rate'] = successful_tests / len(test_images[:3])
        
        return results
    
    def find_test_images(self) -> List[str]:
        """æŸ¥æ‰¾æµ‹è¯•å›¾ç‰‡"""
        test_image_dirs = [
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'demo_images'),
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'backend', 'uploads'),
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'uploads'),
            os.path.join(os.path.dirname(__file__), 'test_images')
        ]
        
        test_images = []
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']
        
        for image_dir in test_image_dirs:
            if os.path.exists(image_dir):
                for file in os.listdir(image_dir):
                    if any(file.lower().endswith(ext) for ext in image_extensions):
                        test_images.append(os.path.join(image_dir, file))
                        if len(test_images) >= 5:  # æœ€å¤š5å¼ 
                            break
            if len(test_images) >= 5:
                break
        
        return test_images
    
    def compare_models(self, qwen3_results: Dict, smolvlm2_results: Dict) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        comparison = {
            'timestamp': datetime.now().isoformat(),
            'performance_comparison': {},
            'quality_comparison': {},
            'recommendation': ''
        }
        
        # æ€§èƒ½æ¯”è¾ƒ
        qwen3_perf = qwen3_results.get('performance_metrics', {})
        smolvlm2_perf = smolvlm2_results.get('performance_metrics', {})
        
        comparison['performance_comparison'] = {
            'response_time': {
                'qwen3_avg': qwen3_perf.get('avg_response_time', 0),
                'smolvlm2_avg': smolvlm2_perf.get('avg_response_time', 0),
                'winner': 'qwen3' if qwen3_perf.get('avg_response_time', float('inf')) < smolvlm2_perf.get('avg_response_time', float('inf')) else 'smolvlm2'
            },
            'success_rate': {
                'qwen3_rate': qwen3_perf.get('success_rate', 0),
                'smolvlm2_rate': smolvlm2_perf.get('success_rate', 0),
                'winner': 'qwen3' if qwen3_perf.get('success_rate', 0) > smolvlm2_perf.get('success_rate', 0) else 'smolvlm2'
            },
            'quality_score': {
                'qwen3_avg': qwen3_perf.get('avg_quality_score', 0),
                'smolvlm2_avg': smolvlm2_perf.get('avg_quality_score', 0),
                'winner': 'qwen3' if qwen3_perf.get('avg_quality_score', 0) > smolvlm2_perf.get('avg_quality_score', 0) else 'smolvlm2'
            }
        }
        
        # ç”Ÿæˆå»ºè®®
        qwen3_score = 0
        smolvlm2_score = 0
        
        # å“åº”æ—¶é—´è¯„åˆ†
        if comparison['performance_comparison']['response_time']['winner'] == 'qwen3':
            qwen3_score += 1
        else:
            smolvlm2_score += 1
        
        # æˆåŠŸç‡è¯„åˆ†
        if comparison['performance_comparison']['success_rate']['winner'] == 'qwen3':
            qwen3_score += 1
        else:
            smolvlm2_score += 1
        
        # è´¨é‡è¯„åˆ†
        if comparison['performance_comparison']['quality_score']['winner'] == 'qwen3':
            qwen3_score += 1
        else:
            smolvlm2_score += 1
        
        # å¤šæ¨¡æ€èƒ½åŠ›
        if 'multimodal_results' in smolvlm2_results:
            smolvlm2_score += 2  # å¤šæ¨¡æ€èƒ½åŠ›åŠ åˆ†
        
        if qwen3_score > smolvlm2_score:
            comparison['recommendation'] = 'Qwen3åœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œæ¨èç”¨äºæ–‡æœ¬åˆ†æä»»åŠ¡'
        elif smolvlm2_score > qwen3_score:
            comparison['recommendation'] = 'SmolVLM2ç»¼åˆè¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯æ”¯æŒå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¨èä½œä¸ºä¸»è¦æ¨¡å‹'
        else:
            comparison['recommendation'] = 'ä¸¤ä¸ªæ¨¡å‹å„æœ‰ä¼˜åŠ¿ï¼Œå»ºè®®æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©ï¼šçº¯æ–‡æœ¬ç”¨Qwen3ï¼Œå›¾æ–‡ç»“åˆç”¨SmolVLM2'
        
        return comparison
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜è¯„æµ‹ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'model_evaluation_results_{timestamp}.json'
        
        filepath = os.path.join(os.path.dirname(__file__), filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def run_comprehensive_evaluation(self):
        """è¿è¡Œç»¼åˆè¯„æµ‹"""
        print("ğŸš€ å¼€å§‹ç»¼åˆæ¨¡å‹è¯„æµ‹")
        print("="*80)
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = self.get_system_info()
        print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯: CPUæ ¸å¿ƒæ•°={system_info['cpu_count']}, å†…å­˜={system_info['memory_total']:.1f}GB")
        print(f"ğŸ–¥ï¸  GPUä¿¡æ¯: CUDAå¯ç”¨={system_info['gpu_info']['cuda_available']}, GPUæ•°é‡={system_info['gpu_info']['gpu_count']}")
        
        evaluation_results = {
            'system_info': system_info,
            'qwen3_available': QWEN3_AVAILABLE,
            'smolvlm2_available': SMOLVLM2_AVAILABLE
        }
        
        # æµ‹è¯•Qwen3
        if QWEN3_AVAILABLE:
            try:
                print("\nğŸ”§ åˆå§‹åŒ–Qwen3æ¨¡å‹...")
                self.qwen3_client = Qwen3Client(device="auto", quantization="4bit")
                
                # åˆå¹¶æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
                all_test_cases = []
                for category, cases in self.test_cases.items():
                    all_test_cases.extend(cases)
                
                qwen3_results = self.test_model_performance(self.qwen3_client, "Qwen3", all_test_cases)
                evaluation_results['qwen3_results'] = qwen3_results
                
            except Exception as e:
                print(f"âŒ Qwen3æµ‹è¯•å¤±è´¥: {e}")
                evaluation_results['qwen3_results'] = {'error': str(e)}
        else:
            print("\nâš ï¸  è·³è¿‡Qwen3æµ‹è¯•ï¼ˆä¸å¯ç”¨ï¼‰")
        
        # æµ‹è¯•SmolVLM2
        if SMOLVLM2_AVAILABLE:
            try:
                print("\nğŸ”§ åˆå§‹åŒ–SmolVLM2æ¨¡å‹...")
                self.smolvlm2_client = SmolVLM2Client(
                    device="auto",
                    max_tokens=512,
                    temperature=0.7
                )
                
                # æ–‡æœ¬æµ‹è¯•
                all_test_cases = []
                for category, cases in self.test_cases.items():
                    all_test_cases.extend(cases)
                
                smolvlm2_results = self.test_model_performance(self.smolvlm2_client, "SmolVLM2", all_test_cases)
                
                # å¤šæ¨¡æ€æµ‹è¯•
                test_images = self.find_test_images()
                if test_images:
                    multimodal_results = self.test_multimodal_performance(self.smolvlm2_client, test_images)
                    smolvlm2_results['multimodal_results'] = multimodal_results
                
                evaluation_results['smolvlm2_results'] = smolvlm2_results
                
            except Exception as e:
                print(f"âŒ SmolVLM2æµ‹è¯•å¤±è´¥: {e}")
                evaluation_results['smolvlm2_results'] = {'error': str(e)}
        else:
            print("\nâš ï¸  è·³è¿‡SmolVLM2æµ‹è¯•ï¼ˆä¸å¯ç”¨ï¼‰")
        
        # æ¨¡å‹æ¯”è¾ƒ
        if (QWEN3_AVAILABLE and 'qwen3_results' in evaluation_results and 'error' not in evaluation_results['qwen3_results'] and
            SMOLVLM2_AVAILABLE and 'smolvlm2_results' in evaluation_results and 'error' not in evaluation_results['smolvlm2_results']):
            
            comparison_results = self.compare_models(
                evaluation_results['qwen3_results'],
                evaluation_results['smolvlm2_results']
            )
            evaluation_results['comparison'] = comparison_results
        
        # æ‰“å°æ€»ç»“
        self.print_evaluation_summary(evaluation_results)
        
        # ä¿å­˜ç»“æœ
        self.save_results(evaluation_results)
        
        return evaluation_results
    
    def print_evaluation_summary(self, results: Dict[str, Any]):
        """æ‰“å°è¯„æµ‹æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ¯ ç»¼åˆè¯„æµ‹æ€»ç»“")
        print("="*80)
        
        # Qwen3ç»“æœ
        if 'qwen3_results' in results and 'error' not in results['qwen3_results']:
            qwen3_perf = results['qwen3_results']['performance_metrics']
            print(f"\nğŸ“Š Qwen3æ€§èƒ½æŒ‡æ ‡:")
            print(f"  âœ… æˆåŠŸç‡: {qwen3_perf['success_rate']:.1%}")
            print(f"  â±ï¸  å¹³å‡å“åº”æ—¶é—´: {qwen3_perf['avg_response_time']:.2f}ç§’")
            print(f"  ğŸ“„ å¹³å‡å“åº”é•¿åº¦: {qwen3_perf['avg_response_length']:.0f}å­—ç¬¦")
            print(f"  ğŸ¯ å¹³å‡è´¨é‡è¯„åˆ†: {qwen3_perf['avg_quality_score']:.1f}/100")
            print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {qwen3_perf['resource_usage']['avg_cpu_usage']:.1f}%")
        
        # SmolVLM2ç»“æœ
        if 'smolvlm2_results' in results and 'error' not in results['smolvlm2_results']:
            smolvlm2_perf = results['smolvlm2_results']['performance_metrics']
            print(f"\nğŸ“Š SmolVLM2æ€§èƒ½æŒ‡æ ‡:")
            print(f"  âœ… æ–‡æœ¬æˆåŠŸç‡: {smolvlm2_perf['success_rate']:.1%}")
            print(f"  â±ï¸  æ–‡æœ¬å¹³å‡å“åº”æ—¶é—´: {smolvlm2_perf['avg_response_time']:.2f}ç§’")
            print(f"  ğŸ“„ æ–‡æœ¬å¹³å‡å“åº”é•¿åº¦: {smolvlm2_perf['avg_response_length']:.0f}å­—ç¬¦")
            print(f"  ğŸ¯ æ–‡æœ¬å¹³å‡è´¨é‡è¯„åˆ†: {smolvlm2_perf['avg_quality_score']:.1f}/100")
            print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {smolvlm2_perf['resource_usage']['avg_cpu_usage']:.1f}%")
            
            if 'multimodal_results' in results['smolvlm2_results']:
                mm_results = results['smolvlm2_results']['multimodal_results']
                print(f"  ğŸ–¼ï¸  å¤šæ¨¡æ€æˆåŠŸç‡: {mm_results['success_rate']:.1%}")
                print(f"  â±ï¸  å¤šæ¨¡æ€å¹³å‡å“åº”æ—¶é—´: {mm_results['avg_response_time']:.2f}ç§’")
        
        # æ¯”è¾ƒç»“æœ
        if 'comparison' in results:
            comparison = results['comparison']
            print(f"\nğŸ† æ¨¡å‹æ¯”è¾ƒç»“æœ:")
            perf_comp = comparison['performance_comparison']
            print(f"  âš¡ å“åº”é€Ÿåº¦ä¼˜èƒœè€…: {perf_comp['response_time']['winner'].upper()}")
            print(f"  âœ… æˆåŠŸç‡ä¼˜èƒœè€…: {perf_comp['success_rate']['winner'].upper()}")
            print(f"  ğŸ¯ è´¨é‡è¯„åˆ†ä¼˜èƒœè€…: {perf_comp['quality_score']['winner'].upper()}")
            print(f"\nğŸ’¡ æ¨è: {comparison['recommendation']}")
        
        print("\nâœ… ç»¼åˆè¯„æµ‹å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    try:
        evaluator = ComprehensiveModelEvaluator()
        results = evaluator.run_comprehensive_evaluation()
        return 0
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())