#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„æŸ¥è¯¢å¤„ç†å™¨
å®ç°å›¾ç‰‡+æ–‡å­—æŸ¥è¯¢çš„ä¼˜åŒ–æµç¨‹ï¼š
1. å›¾ç‰‡å…ˆæŸ¥è¯¢Milvusè·å–top5ç›¸ä¼¼ç»“æœ
2. æ£€æŸ¥ç›¸ä¼¼åº¦æ˜¯å¦è¾¾åˆ°æ ‡å‡†
3. å¦‚æœè¾¾æ ‡åˆ™ä½¿ç”¨metadataä½œä¸ºpromptçš„ä¸€éƒ¨åˆ†æŸ¥è¯¢å¤§æ¨¡å‹
"""

import os
import sys
import json
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple
from PIL import Image
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.logger_config import get_ai_logger

# æ·»åŠ ai_coreè·¯å¾„
ai_core_path = os.path.dirname(__file__)
if ai_core_path not in sys.path:
    sys.path.insert(0, ai_core_path)

from clip_encoder.clip_encoder import CLIPEncoder
from vector_db.milvus_client import MilvusClient
from ai_core.models.client.internvl3_5_client import InternVL3_5Client

# é…ç½®æ—¥å¿—
logger = get_ai_logger('optimized_query_processor')


class OptimizedQueryProcessor:
    """ä¼˜åŒ–çš„æŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self, 
                 similarity_threshold: float = 0.7,
                 top_k: int = 5,
                 clip_encoder=None,
                 milvus_client=None,
                 internvl3_5_client=None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æŸ¥è¯¢å¤„ç†å™¨
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼æ‰ä½¿ç”¨metadataå¢å¼ºprompt
            top_k: ä»Milvusæ£€ç´¢çš„topç»“æœæ•°é‡
            clip_encoder: å¤–éƒ¨ä¼ å…¥çš„CLIPç¼–ç å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            milvus_client: å¤–éƒ¨ä¼ å…¥çš„Milvuså®¢æˆ·ç«¯å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            internvl3_5_client: å¤–éƒ¨ä¼ å…¥çš„InternVL3.5å®¢æˆ·ç«¯å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.similarity_threshold = similarity_threshold
        self.top_k = top_k
        
        # ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ç»„ä»¶æˆ–åˆå§‹åŒ–æ–°ç»„ä»¶
        self.clip_encoder = clip_encoder
        self.milvus_client = milvus_client
        self.internvl3_5_client = internvl3_5_client
        
        # åªåˆå§‹åŒ–æœªæä¾›çš„ç»„ä»¶
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶ï¼ˆåªåˆå§‹åŒ–æœªæä¾›çš„ç»„ä»¶ï¼‰"""
        try:
            # åˆå§‹åŒ–CLIPç¼–ç å™¨ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if self.clip_encoder is None:
                logger.info("åˆå§‹åŒ–CLIPç¼–ç å™¨...")
                self.clip_encoder = CLIPEncoder()
            else:
                logger.info("ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„CLIPç¼–ç å™¨")
            
            # åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if self.milvus_client is None:
                logger.info("åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯...")
                self.milvus_client = MilvusClient()
            else:
                logger.info("ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„Milvuså®¢æˆ·ç«¯")
            
            # åˆå§‹åŒ–InternVL3.5å®¢æˆ·ç«¯ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if self.internvl3_5_client is None:
                logger.info("åˆå§‹åŒ–InternVL3.5å®¢æˆ·ç«¯...")
                self.internvl3_5_client = InternVL3_5Client()
            else:
                logger.info("ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„InternVL3.5å®¢æˆ·ç«¯")
            
            logger.info("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def process_image_text_query(self, 
                                image: Union[str, Image.Image, np.ndarray],
                                text_query: str = "") -> Dict[str, Any]:
        """
        å¤„ç†å›¾ç‰‡+æ–‡å­—æŸ¥è¯¢çš„ä¼˜åŒ–æµç¨‹
        
        Args:
            image: è¾“å…¥å›¾ç‰‡
            text_query: æ–‡å­—æŸ¥è¯¢å†…å®¹
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            logger.info("ğŸš€ å¼€å§‹å¤„ç†å›¾ç‰‡+æ–‡å­—æŸ¥è¯¢")
            logger.info(f"ğŸ“ åŸå§‹æŸ¥è¯¢: {text_query if text_query else '(æ— æ–‡å­—æŸ¥è¯¢)'}")
            
            # æ­¥éª¤1: ä½¿ç”¨CLIPç¼–ç å›¾ç‰‡
            logger.info("ğŸ” æ­¥éª¤1: ç¼–ç å›¾ç‰‡ç‰¹å¾")
            image_features = self.clip_encoder.encode_image(image)
            logger.info(f"âœ… å›¾ç‰‡ç‰¹å¾ç¼–ç å®Œæˆï¼Œç»´åº¦: {len(image_features)}")
            
            # æ­¥éª¤2: åœ¨Milvusä¸­æœç´¢top5ç›¸ä¼¼å›¾ç‰‡
            logger.info(f"ğŸ” æ­¥éª¤2: åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢top{self.top_k}ç›¸ä¼¼å›¾ç‰‡")
            similar_results = self.milvus_client.search_similar_images(
                query_vector=image_features.tolist(),
                top_k=self.top_k,
                score_threshold=0.3  # è®¾ç½®è¾ƒä½é˜ˆå€¼ä»¥è·å–æ›´å¤šå€™é€‰
            )
            
            # æ­¥éª¤3: æ£€æŸ¥æœ€é«˜ç›¸ä¼¼åº¦æ˜¯å¦è¾¾åˆ°æ ‡å‡†
            use_metadata_enhancement = False
            best_match = None
            metadata_context = ""
            
            if similar_results:
                best_match = similar_results[0]
                best_similarity = best_match.get('score', 0)
                
                logger.info(f"ğŸ“Š ç›¸ä¼¼åº¦æ£€æŸ¥ç»“æœ:")
                logger.info(f"   - æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡æ•°é‡: {len(similar_results)}")
                logger.info(f"   - æœ€é«˜ç›¸ä¼¼åº¦: {best_similarity:.4f}")
                logger.info(f"   - è®¾å®šé˜ˆå€¼: {self.similarity_threshold}")
                logger.info(f"   - æ˜¯å¦è¾¾æ ‡: {'âœ… æ˜¯' if best_similarity >= self.similarity_threshold else 'âŒ å¦'}")
                
                # æ‰“å°æ‰€æœ‰ç›¸ä¼¼ç»“æœçš„è¯¦ç»†ä¿¡æ¯
                for i, result in enumerate(similar_results, 1):
                    score = result.get('score', 0)
                    metadata = result.get('metadata', {})
                    name = metadata.get('name', 'æœªçŸ¥') if isinstance(metadata, dict) else 'æœªçŸ¥'
                    logger.info(f"   - ç›¸ä¼¼å›¾ç‰‡{i}: {name} (ç›¸ä¼¼åº¦: {score:.4f})")
                
                if best_similarity >= self.similarity_threshold:
                    use_metadata_enhancement = True
                    logger.info("ğŸ¯ ç›¸ä¼¼åº¦è¾¾æ ‡ï¼Œå¯ç”¨metadataå¢å¼ºåˆ†æ")
                    
                    # æ„å»ºmetadataä¸Šä¸‹æ–‡
                    metadata_context = self._build_metadata_context(similar_results)
                    logger.info(f"ğŸ“‹ æ„å»ºçš„metadataä¸Šä¸‹æ–‡é•¿åº¦: {len(metadata_context)}å­—ç¬¦")
                else:
                    logger.info("âš ï¸ ç›¸ä¼¼åº¦æœªè¾¾æ ‡ï¼Œä½¿ç”¨æ ‡å‡†åˆ†æ")
            else:
                logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•ç›¸ä¼¼å›¾ç‰‡")
            
            # æ­¥éª¤4: æ ¹æ®ç›¸ä¼¼åº¦ç»“æœé€‰æ‹©åˆ†æç­–ç•¥
            logger.info(f"ğŸ”„ æ­¥éª¤4: é€‰æ‹©åˆ†æç­–ç•¥ - {'metadataå¢å¼º' if use_metadata_enhancement else 'æ ‡å‡†åˆ†æ'}")
            
            if use_metadata_enhancement:
                analysis_result = self._analyze_with_metadata_enhancement(
                    image, text_query, metadata_context, similar_results
                )
            else:
                analysis_result = self._analyze_standard(
                    image, text_query
                )
            
            # æ•´åˆæœ€ç»ˆç»“æœ
            final_result = {
                "query_timestamp": datetime.now().isoformat(),
                "image_features": image_features.tolist(),
                "text_query": text_query,
                "similar_results": similar_results,
                "best_similarity": best_match.get('score', 0) if best_match else 0,
                "similarity_threshold": self.similarity_threshold,
                "metadata_enhanced": use_metadata_enhancement,
                "analysis": analysis_result,
                "processing_strategy": "metadata_enhanced" if use_metadata_enhancement else "standard"
            }
            
            # å¤„ç†å®Œæˆçš„è¯¦ç»†æ—¥å¿—è®°å½•
            logger.info("\n" + "ğŸ‰"*20 + " å¤„ç†å®Œæˆ " + "ğŸ‰"*20)
            logger.info("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
            logger.info(f"   - å¤„ç†ç­–ç•¥: {'metadataå¢å¼º' if use_metadata_enhancement else 'æ ‡å‡†åˆ†æ'}")
            logger.info(f"   - ç›¸ä¼¼å›¾ç‰‡æ•°é‡: {len(similar_results) if similar_results else 0}")
            if similar_results:
                logger.info(f"   - æœ€é«˜ç›¸ä¼¼åº¦: {similar_results[0].get('score', 0):.4f}")
            logger.info(f"   - ç»“æœåŒ…å«å­—æ®µ: {list(final_result.keys())}")
            if analysis_result:
                logger.info(f"   - åˆ†ææ–‡æœ¬é•¿åº¦: {len(analysis_result.get('analysis_text', ''))}å­—ç¬¦")
                logger.info(f"   - ä½¿ç”¨æ¨¡å‹: {analysis_result.get('model_used', 'æœªçŸ¥')}")
            else:
                logger.warning("   - åˆ†æç»“æœä¸ºç©º")
            logger.info("="*60)
            
            return final_result
            
        except Exception as e:
            logger.error("\n" + "âŒ"*20 + " å¤„ç†å¤±è´¥ " + "âŒ"*20)
            logger.error(f"ğŸ’¥ é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            logger.error("="*60)
            
            # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                'error': True,
                'error_message': str(e),
                'error_type': type(e).__name__,
                'processing_failed': True,
                'query_timestamp': datetime.now().isoformat(),
                'text_query': text_query if 'text_query' in locals() else '',
                'processing_strategy': 'failed'
            }
    
    def _build_metadata_context(self, similar_results: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºmetadataä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            similar_results: ç›¸ä¼¼æœç´¢ç»“æœ
            
        Returns:
            æ„å»ºçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not similar_results:
            logger.warning("âš ï¸ æ²¡æœ‰ç›¸ä¼¼ç»“æœï¼Œæ— æ³•æ„å»ºmetadataä¸Šä¸‹æ–‡")
            return ""
        
        logger.info(f"ğŸ“‹ å¼€å§‹æ„å»ºmetadataä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨å‰{min(3, len(similar_results))}ä¸ªæœ€ç›¸ä¼¼ç»“æœ")
        
        context_parts = []
        context_parts.append("\n=== ç›¸ä¼¼å¤è‘£å‚è€ƒä¿¡æ¯ ===")
        
        valid_references = 0
        for i, result in enumerate(similar_results[:3], 1):  # åªä½¿ç”¨å‰3ä¸ªæœ€ç›¸ä¼¼çš„
            metadata = result.get('metadata', {})
            similarity = result.get('score', 0)
            
            logger.info(f"ğŸ” å¤„ç†ç›¸ä¼¼ç»“æœ{i}:")
            logger.info(f"   - ç›¸ä¼¼åº¦: {similarity:.4f}")
            logger.info(f"   - metadataç±»å‹: {type(metadata)}")
            logger.info(f"   - metadataå†…å®¹: {metadata}")
            logger.info(f"   - metadataé”®: {list(metadata.keys()) if isinstance(metadata, dict) else 'N/A'}")
            
            context_parts.append(f"\nã€å‚è€ƒå¤è‘£ {i}ã€‘ï¼ˆç›¸ä¼¼åº¦: {similarity:.3f}ï¼‰")
            
            # æå–å…³é”®metadataä¿¡æ¯
            if isinstance(metadata, dict):
                # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
                name = (metadata.get('name') or metadata.get('title') or 
                       metadata.get('artifact_name') or metadata.get('æ–‡ç‰©åç§°') or 'æœªçŸ¥')
                dynasty = (metadata.get('dynasty') or metadata.get('period') or 
                          metadata.get('era') or metadata.get('æœä»£') or 'æœªçŸ¥æœä»£')
                category = (metadata.get('category') or metadata.get('type') or 
                           metadata.get('classification') or metadata.get('ç±»åˆ«') or 'æœªçŸ¥ç±»åˆ«')
                material = (metadata.get('material') or metadata.get('materials') or 
                           metadata.get('composition') or metadata.get('æè´¨') or 'æœªçŸ¥æè´¨')
                description = (metadata.get('description') or metadata.get('desc') or 
                              metadata.get('details') or metadata.get('æè¿°') or 'æ— æè¿°')
                
                logger.info(f"   - åç§°: {name}")
                logger.info(f"   - æœä»£: {dynasty}")
                logger.info(f"   - ç±»åˆ«: {category}")
                logger.info(f"   - æè´¨: {material}")
                logger.info(f"   - æè¿°é•¿åº¦: {len(description)}å­—ç¬¦")
                
                context_parts.append(f"åç§°: {name}")
                context_parts.append(f"æœä»£: {dynasty}")
                context_parts.append(f"ç±»åˆ«: {category}")
                context_parts.append(f"æè´¨: {material}")
                context_parts.append(f"æè¿°: {description}")
                valid_references += 1
            else:
                logger.warning(f"   - âš ï¸ metadataæ ¼å¼æ— æ•ˆ: {metadata}")
            
        context_parts.append("\n=== å‚è€ƒä¿¡æ¯ç»“æŸ ===")
        
        context = "\n".join(context_parts)
        logger.info(f"âœ… metadataä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ:")
        logger.info(f"   - æœ‰æ•ˆå‚è€ƒæ•°é‡: {valid_references}")
        logger.info(f"   - ä¸Šä¸‹æ–‡æ€»é•¿åº¦: {len(context)}å­—ç¬¦")
        logger.info(f"   - ä¸Šä¸‹æ–‡è¡Œæ•°: {len(context.split())}è¡Œ")
        
        return context
    
    def _analyze_with_metadata_enhancement(self, 
                                         image: Union[str, Image.Image, np.ndarray],
                                         text_query: str,
                                         metadata_context: str,
                                         similar_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ä½¿ç”¨metadataå¢å¼ºçš„åˆ†æ
        
        Args:
            image: è¾“å…¥å›¾ç‰‡
            text_query: æ–‡å­—æŸ¥è¯¢
            metadata_context: metadataä¸Šä¸‹æ–‡
            similar_results: ç›¸ä¼¼æœç´¢ç»“æœ
            
        Returns:
            åˆ†æç»“æœ
        """
        logger.info("ğŸ¯ å¼€å§‹metadataå¢å¼ºåˆ†æ")
        
        # æ„å»ºå¢å¼ºçš„prompt
        enhanced_prompt = self._build_enhanced_prompt(text_query, metadata_context)
        
        # è®°å½•promptä¼˜åŒ–å¯¹æ¯”
        original_prompt = text_query if text_query else "è¯·åˆ†æè¿™ä»¶å¤è‘£"
        logger.info("\n" + "="*80)
        logger.info("ğŸ“‹ PROMPTä¼˜åŒ–å¯¹æ¯”:")
        logger.info("="*80)
        logger.info(f"ğŸ”¸ åŸå§‹prompt ({len(original_prompt)}å­—ç¬¦):")
        logger.info(f"   {original_prompt}")
        logger.info("-"*40)
        logger.info(f"ğŸ”¹ å¢å¼ºprompt ({len(enhanced_prompt)}å­—ç¬¦):")
        # åˆ†è¡Œæ˜¾ç¤ºå¢å¼ºpromptï¼Œé¿å…è¿‡é•¿
        for i, line in enumerate(enhanced_prompt.split('\n')[:10], 1):
            if line.strip():
                logger.info(f"   {i:2d}| {line[:100]}{'...' if len(line) > 100 else ''}")
        if len(enhanced_prompt.split('\n')) > 10:
            remaining_lines = len(enhanced_prompt.split('\n')) - 10
            logger.info(f"   ... (è¿˜æœ‰{remaining_lines}è¡Œ)")
        logger.info("="*80)
        
        # ä½¿ç”¨InternVL3.5è¿›è¡Œåˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰
        logger.info("ğŸ¤– è°ƒç”¨InternVL3.5è¿›è¡Œå¢å¼ºåˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰...")
        analysis_text = ""
        for chunk in self.internvl3_5_client.chat_about_antique_stream(enhanced_prompt, image):
            analysis_text += chunk
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®æ—¶è¾“å‡ºé€»è¾‘
            logger.debug(f"æµå¼è¾“å‡º: {chunk}")
        model_used = "InternVL3.5"
        
        logger.info("âœ… metadataå¢å¼ºåˆ†æå®Œæˆ")
        logger.info(f"ğŸ“Š åˆ†æç»“æœé•¿åº¦: {len(analysis_text)}å­—ç¬¦")
        
        return {
            "analysis_text": analysis_text,
            "model_used": model_used,
            "enhancement_type": "metadata_enhanced",
            "reference_count": len(similar_results),
            "prompt_used": enhanced_prompt,
            "prompt_enhancement": {
                "original_length": len(original_prompt),
                "enhanced_length": len(enhanced_prompt),
                "enhancement_ratio": len(enhanced_prompt) / len(original_prompt) if original_prompt else 0
            }
        }
        
    def _build_enhanced_prompt(self, text_query: str, metadata_context: str) -> str:
        """
        æ„å»ºå¢å¼ºçš„prompt
        
        Args:
            text_query: ç”¨æˆ·æ–‡å­—æŸ¥è¯¢
            metadata_context: metadataä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            æ„å»ºçš„å¢å¼ºprompt
        """
        logger.info("ğŸ”§ å¼€å§‹æ„å»ºå¢å¼ºprompt")
        logger.info(f"   - åŸå§‹æŸ¥è¯¢é•¿åº¦: {len(text_query) if text_query else 0}å­—ç¬¦")
        logger.info(f"   - metadataä¸Šä¸‹æ–‡é•¿åº¦: {len(metadata_context)}å­—ç¬¦")
        
        user_query = text_query if text_query else "è¯·åˆ†æè¿™ä»¶å¤è‘£"
        
        enhanced_prompt = f"""
ä½œä¸ºä¸“ä¸šçš„å¤è‘£é‰´å®šä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å¯¹è¿™ä»¶å¤è‘£è¿›è¡Œè¯¦ç»†åˆ†æï¼š

ç”¨æˆ·æŸ¥è¯¢: {user_query}

{metadata_context}

è¯·ç»“åˆä¸Šè¿°ç›¸ä¼¼å¤è‘£çš„å‚è€ƒä¿¡æ¯ï¼Œä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œä¸“ä¸šåˆ†æï¼š
1. å¤–è§‚ç‰¹å¾å¯¹æ¯”ï¼šä¸å‚è€ƒå¤è‘£çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§
2. æœä»£é£æ ¼åˆ†æï¼šåŸºäºå‚è€ƒä¿¡æ¯æ¨æµ‹å¯èƒ½çš„å†å²æ—¶æœŸ
3. å·¥è‰ºæŠ€æ³•è¯„ä¼°ï¼šåˆ†æåˆ¶ä½œå·¥è‰ºå’ŒæŠ€æœ¯ç‰¹ç‚¹
4. æè´¨é‰´å®šï¼šç»“åˆå‚è€ƒä¿¡æ¯åˆ†æå¯èƒ½çš„æè´¨
5. çœŸä¼ªåˆåˆ¤ï¼šåŸºäºé£æ ¼å¯¹æ¯”è¿›è¡ŒçœŸä¼ªè¯„ä¼°
6. ä»·å€¼è¯„ä¼°ï¼šå‚è€ƒåŒç±»å¤è‘£ç»™å‡ºä»·å€¼åˆ¤æ–­
7. æ”¶è—å»ºè®®ï¼šæä¾›ä¸“ä¸šçš„æ”¶è—å’Œä¿å…»å»ºè®®

è¯·æä¾›è¯¦ç»†ã€ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚
"""
        
        logger.info("âœ… å¢å¼ºpromptæ„å»ºå®Œæˆ")
        logger.info(f"   - æœ€ç»ˆprompté•¿åº¦: {len(enhanced_prompt)}å­—ç¬¦")
        logger.info(f"   - å¢å¼ºæ¯”ä¾‹: {len(enhanced_prompt) / len(user_query):.2f}x")
        logger.info(f"   - åŒ…å«å‚è€ƒä¿¡æ¯: {'æ˜¯' if metadata_context else 'å¦'}")
        
        return enhanced_prompt
    
    def _analyze_standard(self, 
                         image: Union[str, Image.Image, np.ndarray],
                         text_query: str) -> Dict[str, Any]:
        """
        æ ‡å‡†åˆ†æï¼ˆæ— metadataå¢å¼ºï¼‰
        
        Args:
            image: è¾“å…¥å›¾ç‰‡
            text_query: æ–‡å­—æŸ¥è¯¢
            
        Returns:
            åˆ†æç»“æœ
        """
        logger.info("âš¡ å¼€å§‹æ ‡å‡†åˆ†æ")
        
        # æ„å»ºæ ‡å‡†prompt
        standard_prompt = f"""
ä½œä¸ºä¸“ä¸šçš„å¤è‘£é‰´å®šä¸“å®¶ï¼Œè¯·å¯¹è¿™ä»¶å¤è‘£è¿›è¡Œè¯¦ç»†åˆ†æï¼š

ç”¨æˆ·æŸ¥è¯¢: {text_query if text_query else 'è¯·åˆ†æè¿™ä»¶å¤è‘£'}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œä¸“ä¸šåˆ†æï¼š
1. å¤–è§‚ç‰¹å¾ï¼šæè¿°å¤è‘£çš„å½¢çŠ¶ã€é¢œè‰²ã€çº¹é¥°ç­‰è§†è§‰ç‰¹å¾
2. æè´¨æ¨æµ‹ï¼šåˆ†æå¯èƒ½çš„åˆ¶ä½œææ–™
3. å·¥è‰ºç‰¹å¾ï¼šè§‚å¯Ÿåˆ¶ä½œå·¥è‰ºå’ŒæŠ€æ³•
4. å¹´ä»£æ¨æµ‹ï¼šæ ¹æ®é£æ ¼ç‰¹å¾æ¨æµ‹å¯èƒ½çš„å†å²æ—¶æœŸ
5. çœŸä¼ªè¯„ä¼°ï¼šåŸºäºè§‚å¯Ÿåˆ°çš„ç‰¹å¾è¿›è¡Œåˆæ­¥çœŸä¼ªåˆ¤æ–­
6. ä¿å­˜çŠ¶å†µï¼šè¯„ä¼°å¤è‘£çš„ä¿å­˜çŠ¶æ€
7. æ”¶è—ä»·å€¼ï¼šåˆ†æå…¶æ½œåœ¨çš„æ”¶è—å’Œå¸‚åœºä»·å€¼

è¯·æä¾›ä¸“ä¸šã€è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚
"""
        
        # è®°å½•æ ‡å‡†åˆ†æçš„promptä¿¡æ¯
        original_query = text_query if text_query else "è¯·åˆ†æè¿™ä»¶å¤è‘£"
        logger.info("\n" + "="*60)
        logger.info("ğŸ“ æ ‡å‡†åˆ†æPROMPTä¿¡æ¯:")
        logger.info("="*60)
        logger.info(f"ğŸ”¸ ç”¨æˆ·æŸ¥è¯¢: {original_query}")
        logger.info(f"ğŸ”¹ æ ‡å‡†prompté•¿åº¦: {len(standard_prompt)}å­—ç¬¦")
        logger.info(f"ğŸ”¹ åˆ†æç­–ç•¥: åŸºäºå›¾åƒçš„ç›´æ¥åˆ†æï¼ˆæ— å‚è€ƒä¿¡æ¯ï¼‰")
        logger.info("="*60)
        
        # ä½¿ç”¨InternVL3.5è¿›è¡Œåˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰
        logger.info("ğŸ¤– è°ƒç”¨InternVL3.5è¿›è¡Œæ ‡å‡†åˆ†æï¼ˆæµå¼è¾“å‡ºï¼‰...")
        analysis_text = ""
        for chunk in self.internvl3_5_client.chat_about_antique_stream(standard_prompt, image):
            analysis_text += chunk
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®æ—¶è¾“å‡ºé€»è¾‘
            logger.debug(f"æµå¼è¾“å‡º: {chunk}")
        model_used = "InternVL3.5"
        
        logger.info("âœ… æ ‡å‡†åˆ†æå®Œæˆ")
        logger.info(f"ğŸ“Š åˆ†æç»“æœé•¿åº¦: {len(analysis_text)}å­—ç¬¦")
        
        return {
            "analysis_text": analysis_text,
            "model_used": model_used,
            "enhancement_type": "standard",
            "reference_count": 0,
            "prompt_used": standard_prompt,
            "prompt_info": {
                "original_query": original_query,
                "prompt_length": len(standard_prompt),
                "analysis_strategy": "direct_image_analysis"
            }
        }
    
    def process_image_text_query_stream(self, 
                                       image: Union[str, Image.Image, np.ndarray],
                                       text_query: str = ""):
        """
        æµå¼å¤„ç†å›¾ç‰‡+æ–‡å­—æŸ¥è¯¢çš„ä¼˜åŒ–æµç¨‹
        
        Args:
            image: è¾“å…¥å›¾ç‰‡ï¼ˆæ–‡ä»¶è·¯å¾„ã€PILå›¾åƒæˆ–numpyæ•°ç»„ï¼‰
            text_query: æ–‡å­—æŸ¥è¯¢å†…å®¹
            
        Yields:
            åˆ†æç»“æœæ–‡æœ¬ç‰‡æ®µ
        """
        logger.info(f"ğŸš€ å¼€å§‹æµå¼ä¼˜åŒ–æŸ¥è¯¢å¤„ç† - æŸ¥è¯¢: {text_query[:50]}...")
        
        try:
            # 1. å›¾ç‰‡ç¼–ç 
            logger.info("ğŸ“¸ å¼€å§‹å›¾ç‰‡ç¼–ç ...")
            image_embedding = self.clip_encoder.encode_image(image)
            logger.info(f"âœ… å›¾ç‰‡ç¼–ç å®Œæˆï¼Œå‘é‡ç»´åº¦: {image_embedding.shape}")
            
            # 2. å‘é‡æ•°æ®åº“æ£€ç´¢
            logger.info(f"ğŸ” åœ¨Milvusä¸­æ£€ç´¢top{self.top_k}ç›¸ä¼¼ç»“æœ...")
            similar_results = self.milvus_client.search_similar_images(
                query_vector=image_embedding.tolist(),
                top_k=self.top_k,
                score_threshold=0.3
            )
            
            if not similar_results:
                logger.info("âŒ æœªæ‰¾åˆ°ç›¸ä¼¼ç»“æœï¼Œä½¿ç”¨æ ‡å‡†åˆ†æ")
                yield from self._analyze_standard_stream(image, text_query)
                return
            
            # 3. æ£€æŸ¥ç›¸ä¼¼åº¦
            max_similarity = max([result.get('score', 0) for result in similar_results])
            logger.info(f"ğŸ“Š æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}, é˜ˆå€¼: {self.similarity_threshold}")
            
            if max_similarity >= self.similarity_threshold:
                logger.info("âœ… ç›¸ä¼¼åº¦è¾¾æ ‡ï¼Œä½¿ç”¨metadataå¢å¼ºåˆ†æ")
                yield from self._analyze_with_metadata_enhancement_stream(
                    image, text_query, similar_results
                )
            else:
                logger.info("âš ï¸ ç›¸ä¼¼åº¦æœªè¾¾æ ‡ï¼Œä½¿ç”¨æ ‡å‡†åˆ†æ")
                yield from self._analyze_standard_stream(image, text_query)
                
        except Exception as e:
            logger.error(f"æµå¼ä¼˜åŒ–æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            yield f"æŸ¥è¯¢å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def _analyze_standard_stream(self, 
                               image: Union[str, Image.Image, np.ndarray],
                               text_query: str):
        """
        æµå¼æ ‡å‡†åˆ†æï¼ˆæ— metadataå¢å¼ºï¼‰
        
        Args:
            image: è¾“å…¥å›¾ç‰‡
            text_query: æ–‡å­—æŸ¥è¯¢
            
        Yields:
            åˆ†æç»“æœæ–‡æœ¬ç‰‡æ®µ
        """
        logger.info("âš¡ å¼€å§‹æµå¼æ ‡å‡†åˆ†æ")
        
        # æ„å»ºæ ‡å‡†prompt
        standard_prompt = f"""
ä½œä¸ºä¸“ä¸šçš„å¤è‘£é‰´å®šä¸“å®¶ï¼Œè¯·å¯¹è¿™ä»¶å¤è‘£è¿›è¡Œè¯¦ç»†åˆ†æï¼š

ç”¨æˆ·æŸ¥è¯¢: {text_query if text_query else 'è¯·åˆ†æè¿™ä»¶å¤è‘£'}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œä¸“ä¸šåˆ†æï¼š
1. å¤–è§‚ç‰¹å¾ï¼šæè¿°å¤è‘£çš„å½¢çŠ¶ã€é¢œè‰²ã€çº¹é¥°ç­‰è§†è§‰ç‰¹å¾
2. æè´¨æ¨æµ‹ï¼šåˆ†æå¯èƒ½çš„åˆ¶ä½œææ–™
3. å·¥è‰ºç‰¹å¾ï¼šè§‚å¯Ÿåˆ¶ä½œå·¥è‰ºå’ŒæŠ€æ³•
4. å¹´ä»£æ¨æµ‹ï¼šæ ¹æ®é£æ ¼ç‰¹å¾æ¨æµ‹å¯èƒ½çš„å†å²æ—¶æœŸ
5. çœŸä¼ªè¯„ä¼°ï¼šåŸºäºè§‚å¯Ÿåˆ°çš„ç‰¹å¾è¿›è¡Œåˆæ­¥çœŸä¼ªåˆ¤æ–­
6. ä¿å­˜çŠ¶å†µï¼šè¯„ä¼°å¤è‘£çš„ä¿å­˜çŠ¶æ€
7. æ”¶è—ä»·å€¼ï¼šåˆ†æå…¶æ½œåœ¨çš„æ”¶è—å’Œå¸‚åœºä»·å€¼

è¯·æä¾›ä¸“ä¸šã€è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚
"""
        
        logger.info("ğŸ¤– è°ƒç”¨InternVL3.5è¿›è¡Œæµå¼æ ‡å‡†åˆ†æ...")
        
        # ç›´æ¥æµå¼è¾“å‡º
        for chunk in self.internvl3_5_client.chat_about_antique_stream(standard_prompt, image):
            logger.debug(f"æµå¼è¾“å‡º: {chunk}")
            yield chunk
    
    def _analyze_with_metadata_enhancement_stream(self, 
                                                image: Union[str, Image.Image, np.ndarray],
                                                text_query: str,
                                                similar_results: List[Dict[str, Any]]):
        """
        æµå¼metadataå¢å¼ºåˆ†æ
        
        Args:
            image: è¾“å…¥å›¾ç‰‡
            text_query: æ–‡å­—æŸ¥è¯¢
            similar_results: ç›¸ä¼¼ç»“æœåˆ—è¡¨
            
        Yields:
            åˆ†æç»“æœæ–‡æœ¬ç‰‡æ®µ
        """
        logger.info("ğŸ”¥ å¼€å§‹æµå¼metadataå¢å¼ºåˆ†æ")
        
        # æ„å»ºmetadataä¸Šä¸‹æ–‡
        metadata_context = self._build_metadata_context(similar_results)
        
        # æ„å»ºå¢å¼ºprompt
        enhanced_prompt = self._build_enhanced_prompt(text_query, metadata_context)
        
        logger.info("ğŸ¤– è°ƒç”¨InternVL3.5è¿›è¡Œæµå¼å¢å¼ºåˆ†æ...")
        
        # ç›´æ¥æµå¼è¾“å‡º
        for chunk in self.internvl3_5_client.chat_about_antique_stream(enhanced_prompt, image):
            logger.debug(f"æµå¼è¾“å‡º: {chunk}")
            yield chunk
    
    def get_processor_status(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†å™¨çŠ¶æ€
        
        Returns:
            çŠ¶æ€ä¿¡æ¯å­—å…¸
        """
        return {
            "similarity_threshold": self.similarity_threshold,
            "top_k": self.top_k,
            "clip_encoder_ready": self.clip_encoder is not None,
            "milvus_client_ready": self.milvus_client is not None,
            "internvl3_5_ready": self.internvl3_5_client is not None
        }
    
    def update_similarity_threshold(self, new_threshold: float):
        """
        æ›´æ–°ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Args:
            new_threshold: æ–°çš„ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        old_threshold = self.similarity_threshold
        self.similarity_threshold = new_threshold
        logger.info(f"ç›¸ä¼¼åº¦é˜ˆå€¼å·²æ›´æ–°: {old_threshold} -> {new_threshold}")


def main():
    """æµ‹è¯•å‡½æ•°"""
    try:
        # åˆ›å»ºä¼˜åŒ–æŸ¥è¯¢å¤„ç†å™¨
        processor = OptimizedQueryProcessor(
            similarity_threshold=0.7,
            top_k=5
        )
        
        # è·å–çŠ¶æ€
        status = processor.get_processor_status()
        print("å¤„ç†å™¨çŠ¶æ€:")
        for key, value in status.items():
            print(f"  {key}: {value}")
        
        print("\nä¼˜åŒ–æŸ¥è¯¢å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()