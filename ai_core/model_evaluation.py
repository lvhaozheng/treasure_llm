#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwenå’ŒSmolVLM2æ¨¡å‹æ•ˆæœè¯„æµ‹è„šæœ¬
æ”¯æŒå¤šç»´åº¦è¯„æµ‹ï¼šå“åº”è´¨é‡ã€æ¨ç†é€Ÿåº¦ã€èµ„æºå ç”¨ã€å‡†ç¡®æ€§ç­‰
"""

import os
import sys
import json
import time
import logging
import psutil
import torch
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from PIL import Image
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥æ¨¡å‹å®¢æˆ·ç«¯
from models.client.qwen3_client import Qwen3Client
from models.client.smolvlm2_client import SmolVLM2Client
from utils.logger_config import get_ai_logger

# é…ç½®æ—¥å¿—
logger = get_ai_logger('model_evaluation')

class ModelEvaluator:
    """æ¨¡å‹è¯„æµ‹å™¨"""
    
    def __init__(self, output_dir: str = None):
        """
        åˆå§‹åŒ–æ¨¡å‹è¯„æµ‹å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        """
        self.output_dir = output_dir or os.getcwd()
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ¨¡å‹å®¢æˆ·ç«¯
        self.qwen3_client = None
        self.smolvlm2_client = None
        
        # è¯„æµ‹ç»“æœ
        self.evaluation_results = {
            'timestamp': self.timestamp,
            'system_info': self._get_system_info(),
            'qwen3_results': {},
            'smolvlm2_results': {},
            'comparison': {}
        }
        
        # æµ‹è¯•æ•°æ®
        self.test_cases = self._prepare_test_cases()
        
        logger.info(f"æ¨¡å‹è¯„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total // (1024**3),  # GB
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'cuda_device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            'python_version': sys.version,
            'torch_version': torch.__version__
        }
    
    def _prepare_test_cases(self) -> Dict[str, List[Dict]]:
        """å‡†å¤‡æµ‹è¯•ç”¨ä¾‹"""
        return {
            'text_analysis': [
                {
                    'id': 'text_001',
                    'prompt': 'è¯·åˆ†æè¿™ä»¶é’èŠ±ç“·å™¨çš„ç‰¹å¾å’Œä»·å€¼',
                    'expected_keywords': ['é’èŠ±', 'ç“·å™¨', 'ä»·å€¼', 'ç‰¹å¾'],
                    'difficulty': 'easy'
                },
                {
                    'id': 'text_002', 
                    'prompt': 'è¿™æ˜¯ä¸€ä»¶æ˜ä»£å®£å¾·å¹´é—´çš„é’èŠ±ç¼ æè²çº¹æ¢…ç“¶ï¼Œè¯·è¯¦ç»†åˆ†æå…¶å·¥è‰ºç‰¹ç‚¹ã€å†å²ä»·å€¼å’Œå¸‚åœºå‰æ™¯',
                    'expected_keywords': ['æ˜ä»£', 'å®£å¾·', 'é’èŠ±', 'ç¼ æè²', 'æ¢…ç“¶', 'å·¥è‰º', 'å†å²', 'å¸‚åœº'],
                    'difficulty': 'hard'
                },
                {
                    'id': 'text_003',
                    'prompt': 'è¯·æ¯”è¾ƒå”ä¸‰å½©å’Œå®‹ä»£å®šçª‘ç™½ç“·çš„åŒºåˆ«',
                    'expected_keywords': ['å”ä¸‰å½©', 'å®‹ä»£', 'å®šçª‘', 'ç™½ç“·', 'åŒºåˆ«', 'æ¯”è¾ƒ'],
                    'difficulty': 'medium'
                }
            ],
            'multimodal_analysis': [
                {
                    'id': 'mm_001',
                    'image_path': None,  # å°†åœ¨è¿è¡Œæ—¶æŸ¥æ‰¾æµ‹è¯•å›¾ç‰‡
                    'prompt': 'è¯·åˆ†æè¿™ä»¶å¤è‘£çš„å¹´ä»£ã€æè´¨å’Œä»·å€¼',
                    'expected_keywords': ['å¹´ä»£', 'æè´¨', 'ä»·å€¼'],
                    'difficulty': 'medium'
                }
            ],
            'reasoning_tasks': [
                {
                    'id': 'reason_001',
                    'prompt': 'å¦‚æœä¸€ä»¶ç“·å™¨åº•éƒ¨æœ‰"å¤§æ¸…åº·ç†™å¹´åˆ¶"æ¬¾è¯†ï¼Œä½†é‡‰è‰²åç°ä»£ï¼Œèƒè´¨è¾ƒè½»ï¼Œä½ ä¼šå¦‚ä½•åˆ¤æ–­å…¶çœŸä¼ªï¼Ÿ',
                    'expected_keywords': ['æ¬¾è¯†', 'é‡‰è‰²', 'èƒè´¨', 'çœŸä¼ª', 'åˆ¤æ–­'],
                    'difficulty': 'hard'
                }
            ]
        }
    
    def initialize_models(self) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹"""
        logger.info("å¼€å§‹åˆå§‹åŒ–æ¨¡å‹...")
        
        success = True
        
        # åˆå§‹åŒ–Qwen3
        try:
            logger.info("åˆå§‹åŒ–Qwen3æ¨¡å‹...")
            self.qwen3_client = Qwen3Client(
                device="auto",
                quantization="4bit"
            )
            logger.info("âœ… Qwen3æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Qwen3æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            success = False
        
        # åˆå§‹åŒ–SmolVLM2
        try:
            logger.info("åˆå§‹åŒ–SmolVLM2æ¨¡å‹...")
            self.smolvlm2_client = SmolVLM2Client(
                device="auto",
                max_tokens=512,
                temperature=0.7
            )
            logger.info("âœ… SmolVLM2æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ SmolVLM2æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            success = False
        
        return success
    
    def _measure_performance(self, func, *args, **kwargs) -> Tuple[Any, Dict[str, float]]:
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ€§èƒ½"""
        # è®°å½•åˆå§‹çŠ¶æ€
        process = psutil.Process()
        initial_memory = process.memory_info().rss / (1024**2)  # MB
        initial_cpu = process.cpu_percent()
        
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
        
        # æ‰§è¡Œå‡½æ•°
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            result = f"Error: {str(e)}"
            success = False
        end_time = time.time()
        
        # è®°å½•æœ€ç»ˆçŠ¶æ€
        final_memory = process.memory_info().rss / (1024**2)  # MB
        final_cpu = process.cpu_percent()
        
        performance_metrics = {
            'execution_time': end_time - start_time,
            'memory_usage': final_memory - initial_memory,
            'cpu_usage': final_cpu,
            'success': success
        }
        
        if torch.cuda.is_available():
            final_gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
            peak_gpu_memory = torch.cuda.max_memory_allocated() / (1024**2)  # MB
            performance_metrics.update({
                'gpu_memory_usage': final_gpu_memory - initial_gpu_memory,
                'peak_gpu_memory': peak_gpu_memory
            })
        
        return result, performance_metrics
    
    def _evaluate_response_quality(self, response: str, expected_keywords: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        if isinstance(response, str) and response.startswith("Error:"):
            return {
                'keyword_coverage': 0.0,
                'response_length': 0,
                'has_error': True,
                'error_message': response
            }
        
        # å…³é”®è¯è¦†ç›–ç‡
        response_lower = response.lower()
        covered_keywords = [kw for kw in expected_keywords if kw.lower() in response_lower]
        keyword_coverage = len(covered_keywords) / len(expected_keywords) if expected_keywords else 0.0
        
        # å“åº”é•¿åº¦
        response_length = len(response)
        
        # ç®€å•çš„è´¨é‡è¯„åˆ†ï¼ˆåŸºäºé•¿åº¦å’Œå…³é”®è¯è¦†ç›–ç‡ï¼‰
        length_score = min(response_length / 200, 1.0)  # 200å­—ç¬¦ä¸ºæ»¡åˆ†
        quality_score = (keyword_coverage * 0.7 + length_score * 0.3)
        
        return {
            'keyword_coverage': keyword_coverage,
            'covered_keywords': covered_keywords,
            'response_length': response_length,
            'quality_score': quality_score,
            'has_error': False
        }
    
    def evaluate_qwen3(self) -> Dict[str, Any]:
        """è¯„æµ‹Qwen3æ¨¡å‹"""
        logger.info("å¼€å§‹è¯„æµ‹Qwen3æ¨¡å‹...")
        
        if not self.qwen3_client:
            return {'error': 'Qwen3å®¢æˆ·ç«¯æœªåˆå§‹åŒ–'}
        
        results = {
            'model_info': self.qwen3_client.get_model_info(),
            'text_analysis': [],
            'reasoning_tasks': [],
            'performance_summary': {}
        }
        
        all_performance_metrics = []
        
        # æ–‡æœ¬åˆ†ææµ‹è¯•
        logger.info("æ‰§è¡Œæ–‡æœ¬åˆ†ææµ‹è¯•...")
        for test_case in self.test_cases['text_analysis']:
            logger.info(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")
            
            response, performance = self._measure_performance(
                self.qwen3_client.text_chat,
                test_case['prompt'],
                512
            )
            
            quality = self._evaluate_response_quality(
                response, 
                test_case['expected_keywords']
            )
            
            test_result = {
                'test_id': test_case['id'],
                'difficulty': test_case['difficulty'],
                'prompt': test_case['prompt'],
                'response': response,
                'performance': performance,
                'quality': quality
            }
            
            results['text_analysis'].append(test_result)
            all_performance_metrics.append(performance)
        
        # æ¨ç†ä»»åŠ¡æµ‹è¯•
        logger.info("æ‰§è¡Œæ¨ç†ä»»åŠ¡æµ‹è¯•...")
        for test_case in self.test_cases['reasoning_tasks']:
            logger.info(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")
            
            response, performance = self._measure_performance(
                self.qwen3_client.text_chat,
                test_case['prompt'],
                512
            )
            
            quality = self._evaluate_response_quality(
                response,
                test_case['expected_keywords']
            )
            
            test_result = {
                'test_id': test_case['id'],
                'difficulty': test_case['difficulty'],
                'prompt': test_case['prompt'],
                'response': response,
                'performance': performance,
                'quality': quality
            }
            
            results['reasoning_tasks'].append(test_result)
            all_performance_metrics.append(performance)
        
        # è®¡ç®—æ€§èƒ½æ‘˜è¦
        if all_performance_metrics:
            results['performance_summary'] = self._calculate_performance_summary(all_performance_metrics)
        
        logger.info("Qwen3æ¨¡å‹è¯„æµ‹å®Œæˆ")
        return results
    
    def evaluate_smolvlm2(self) -> Dict[str, Any]:
        """è¯„æµ‹SmolVLM2æ¨¡å‹"""
        logger.info("å¼€å§‹è¯„æµ‹SmolVLM2æ¨¡å‹...")
        
        if not self.smolvlm2_client:
            return {'error': 'SmolVLM2å®¢æˆ·ç«¯æœªåˆå§‹åŒ–'}
        
        results = {
            'model_info': self.smolvlm2_client.get_model_info(),
            'text_analysis': [],
            'multimodal_analysis': [],
            'reasoning_tasks': [],
            'performance_summary': {}
        }
        
        all_performance_metrics = []
        
        # æ–‡æœ¬åˆ†ææµ‹è¯•
        logger.info("æ‰§è¡Œæ–‡æœ¬åˆ†ææµ‹è¯•...")
        for test_case in self.test_cases['text_analysis']:
            logger.info(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")
            
            response, performance = self._measure_performance(
                self.smolvlm2_client.text_chat,
                test_case['prompt']
            )
            
            quality = self._evaluate_response_quality(
                response,
                test_case['expected_keywords']
            )
            
            test_result = {
                'test_id': test_case['id'],
                'difficulty': test_case['difficulty'],
                'prompt': test_case['prompt'],
                'response': response,
                'performance': performance,
                'quality': quality
            }
            
            results['text_analysis'].append(test_result)
            all_performance_metrics.append(performance)
        
        # å¤šæ¨¡æ€åˆ†ææµ‹è¯•ï¼ˆå¦‚æœæœ‰æµ‹è¯•å›¾ç‰‡ï¼‰
        logger.info("æ‰§è¡Œå¤šæ¨¡æ€åˆ†ææµ‹è¯•...")
        test_images = self._find_test_images()
        
        for test_case in self.test_cases['multimodal_analysis']:
            if test_images:
                test_image = test_images[0]  # ä½¿ç”¨ç¬¬ä¸€å¼ æµ‹è¯•å›¾ç‰‡
                logger.info(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']} with image: {test_image}")
                
                try:
                    image = Image.open(test_image)
                    response, performance = self._measure_performance(
                        self.smolvlm2_client.chat_about_antique,
                        image,
                        test_case['prompt']
                    )
                    
                    quality = self._evaluate_response_quality(
                        response,
                        test_case['expected_keywords']
                    )
                    
                    test_result = {
                        'test_id': test_case['id'],
                        'difficulty': test_case['difficulty'],
                        'image_path': test_image,
                        'prompt': test_case['prompt'],
                        'response': response,
                        'performance': performance,
                        'quality': quality
                    }
                    
                    results['multimodal_analysis'].append(test_result)
                    all_performance_metrics.append(performance)
                    
                except Exception as e:
                    logger.error(f"å¤šæ¨¡æ€æµ‹è¯•å¤±è´¥: {e}")
                    results['multimodal_analysis'].append({
                        'test_id': test_case['id'],
                        'error': str(e)
                    })
            else:
                logger.warning("æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡ï¼Œè·³è¿‡å¤šæ¨¡æ€æµ‹è¯•")
        
        # æ¨ç†ä»»åŠ¡æµ‹è¯•
        logger.info("æ‰§è¡Œæ¨ç†ä»»åŠ¡æµ‹è¯•...")
        for test_case in self.test_cases['reasoning_tasks']:
            logger.info(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")
            
            response, performance = self._measure_performance(
                self.smolvlm2_client.text_chat,
                test_case['prompt']
            )
            
            quality = self._evaluate_response_quality(
                response,
                test_case['expected_keywords']
            )
            
            test_result = {
                'test_id': test_case['id'],
                'difficulty': test_case['difficulty'],
                'prompt': test_case['prompt'],
                'response': response,
                'performance': performance,
                'quality': quality
            }
            
            results['reasoning_tasks'].append(test_result)
            all_performance_metrics.append(performance)
        
        # è®¡ç®—æ€§èƒ½æ‘˜è¦
        if all_performance_metrics:
            results['performance_summary'] = self._calculate_performance_summary(all_performance_metrics)
        
        logger.info("SmolVLM2æ¨¡å‹è¯„æµ‹å®Œæˆ")
        return results
    
    def _find_test_images(self) -> List[str]:
        """æŸ¥æ‰¾æµ‹è¯•å›¾ç‰‡"""
        test_image_dirs = [
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'demo_images'),
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'backend', 'uploads'),
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'uploads')
        ]
        
        test_images = []
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']
        
        for image_dir in test_image_dirs:
            if os.path.exists(image_dir):
                for file in os.listdir(image_dir):
                    if any(file.lower().endswith(ext) for ext in image_extensions):
                        test_images.append(os.path.join(image_dir, file))
        
        return test_images[:3]  # æœ€å¤šè¿”å›3å¼ æµ‹è¯•å›¾ç‰‡
    
    def _calculate_performance_summary(self, metrics_list: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—æ€§èƒ½æ‘˜è¦"""
        if not metrics_list:
            return {}
        
        # è¿‡æ»¤æˆåŠŸçš„æµ‹è¯•
        successful_metrics = [m for m in metrics_list if m.get('success', True)]
        
        if not successful_metrics:
            return {'success_rate': 0.0}
        
        summary = {
            'success_rate': len(successful_metrics) / len(metrics_list),
            'avg_execution_time': np.mean([m['execution_time'] for m in successful_metrics]),
            'max_execution_time': np.max([m['execution_time'] for m in successful_metrics]),
            'min_execution_time': np.min([m['execution_time'] for m in successful_metrics]),
            'avg_memory_usage': np.mean([m.get('memory_usage', 0) for m in successful_metrics]),
            'avg_cpu_usage': np.mean([m.get('cpu_usage', 0) for m in successful_metrics])
        }
        
        # GPUç›¸å…³æŒ‡æ ‡
        gpu_metrics = [m for m in successful_metrics if 'gpu_memory_usage' in m]
        if gpu_metrics:
            summary.update({
                'avg_gpu_memory_usage': np.mean([m['gpu_memory_usage'] for m in gpu_metrics]),
                'max_peak_gpu_memory': np.max([m.get('peak_gpu_memory', 0) for m in gpu_metrics])
            })
        
        return summary
    
    def compare_models(self) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        logger.info("å¼€å§‹æ¨¡å‹æ¯”è¾ƒåˆ†æ...")
        
        qwen3_results = self.evaluation_results.get('qwen3_results', {})
        smolvlm2_results = self.evaluation_results.get('smolvlm2_results', {})
        
        comparison = {
            'performance_comparison': {},
            'quality_comparison': {},
            'capability_comparison': {},
            'recommendations': []
        }
        
        # æ€§èƒ½æ¯”è¾ƒ
        qwen3_perf = qwen3_results.get('performance_summary', {})
        smolvlm2_perf = smolvlm2_results.get('performance_summary', {})
        
        if qwen3_perf and smolvlm2_perf:
            comparison['performance_comparison'] = {
                'speed': {
                    'qwen3_avg_time': qwen3_perf.get('avg_execution_time', 0),
                    'smolvlm2_avg_time': smolvlm2_perf.get('avg_execution_time', 0),
                    'winner': 'qwen3' if qwen3_perf.get('avg_execution_time', float('inf')) < smolvlm2_perf.get('avg_execution_time', float('inf')) else 'smolvlm2'
                },
                'memory_efficiency': {
                    'qwen3_avg_memory': qwen3_perf.get('avg_memory_usage', 0),
                    'smolvlm2_avg_memory': smolvlm2_perf.get('avg_memory_usage', 0),
                    'winner': 'qwen3' if qwen3_perf.get('avg_memory_usage', float('inf')) < smolvlm2_perf.get('avg_memory_usage', float('inf')) else 'smolvlm2'
                },
                'success_rate': {
                    'qwen3_success_rate': qwen3_perf.get('success_rate', 0),
                    'smolvlm2_success_rate': smolvlm2_perf.get('success_rate', 0),
                    'winner': 'qwen3' if qwen3_perf.get('success_rate', 0) > smolvlm2_perf.get('success_rate', 0) else 'smolvlm2'
                }
            }
        
        # è´¨é‡æ¯”è¾ƒ
        qwen3_text_results = qwen3_results.get('text_analysis', [])
        smolvlm2_text_results = smolvlm2_results.get('text_analysis', [])
        
        if qwen3_text_results and smolvlm2_text_results:
            qwen3_avg_quality = np.mean([r['quality']['quality_score'] for r in qwen3_text_results if 'quality' in r and 'quality_score' in r['quality']])
            smolvlm2_avg_quality = np.mean([r['quality']['quality_score'] for r in smolvlm2_text_results if 'quality' in r and 'quality_score' in r['quality']])
            
            comparison['quality_comparison'] = {
                'qwen3_avg_quality': qwen3_avg_quality,
                'smolvlm2_avg_quality': smolvlm2_avg_quality,
                'winner': 'qwen3' if qwen3_avg_quality > smolvlm2_avg_quality else 'smolvlm2'
            }
        
        # èƒ½åŠ›æ¯”è¾ƒ
        comparison['capability_comparison'] = {
            'text_analysis': {
                'qwen3': len(qwen3_results.get('text_analysis', [])) > 0,
                'smolvlm2': len(smolvlm2_results.get('text_analysis', [])) > 0
            },
            'multimodal_analysis': {
                'qwen3': False,  # Qwen3ä¸æ”¯æŒå¤šæ¨¡æ€
                'smolvlm2': len(smolvlm2_results.get('multimodal_analysis', [])) > 0
            },
            'reasoning_tasks': {
                'qwen3': len(qwen3_results.get('reasoning_tasks', [])) > 0,
                'smolvlm2': len(smolvlm2_results.get('reasoning_tasks', [])) > 0
            }
        }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if comparison['performance_comparison']:
            speed_winner = comparison['performance_comparison']['speed']['winner']
            memory_winner = comparison['performance_comparison']['memory_efficiency']['winner']
            
            if speed_winner == memory_winner:
                recommendations.append(f"{speed_winner}åœ¨é€Ÿåº¦å’Œå†…å­˜æ•ˆç‡æ–¹é¢éƒ½è¡¨ç°æ›´å¥½")
            else:
                recommendations.append(f"{speed_winner}é€Ÿåº¦æ›´å¿«ï¼Œ{memory_winner}å†…å­˜æ•ˆç‡æ›´é«˜")
        
        if comparison['capability_comparison']['multimodal_analysis']['smolvlm2']:
            recommendations.append("SmolVLM2æ”¯æŒå¤šæ¨¡æ€åˆ†æï¼Œé€‚åˆå›¾æ–‡ç»“åˆçš„å¤è‘£é‰´å®šä»»åŠ¡")
        
        if comparison['quality_comparison']:
            quality_winner = comparison['quality_comparison']['winner']
            recommendations.append(f"{quality_winner}åœ¨æ–‡æœ¬åˆ†æè´¨é‡æ–¹é¢è¡¨ç°æ›´å¥½")
        
        recommendations.extend([
            "å¯¹äºçº¯æ–‡æœ¬åˆ†æä»»åŠ¡ï¼Œå¯ä»¥æ ¹æ®é€Ÿåº¦å’Œè´¨é‡éœ€æ±‚é€‰æ‹©æ¨¡å‹",
            "å¯¹äºå›¾æ–‡ç»“åˆçš„é‰´å®šä»»åŠ¡ï¼Œå»ºè®®ä½¿ç”¨SmolVLM2",
            "å»ºè®®æ ¹æ®å…·ä½“ä½¿ç”¨åœºæ™¯å’Œèµ„æºé™åˆ¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹"
        ])
        
        comparison['recommendations'] = recommendations
        
        logger.info("æ¨¡å‹æ¯”è¾ƒåˆ†æå®Œæˆ")
        return comparison
    
    def run_evaluation(self) -> str:
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹è¯„æµ‹"""
        logger.info("="*60)
        logger.info("å¼€å§‹Qwenå’ŒSmolVLM2æ¨¡å‹æ•ˆæœè¯„æµ‹")
        logger.info("="*60)
        
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            if not self.initialize_models():
                raise Exception("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            
            # è¯„æµ‹Qwen3
            if self.qwen3_client:
                self.evaluation_results['qwen3_results'] = self.evaluate_qwen3()
            
            # è¯„æµ‹SmolVLM2
            if self.smolvlm2_client:
                self.evaluation_results['smolvlm2_results'] = self.evaluate_smolvlm2()
            
            # æ¨¡å‹æ¯”è¾ƒ
            self.evaluation_results['comparison'] = self.compare_models()
            
            # ä¿å­˜ç»“æœ
            report_file = self._save_evaluation_report()
            
            # æ‰“å°æ‘˜è¦
            self._print_evaluation_summary()
            
            logger.info("="*60)
            logger.info("æ¨¡å‹è¯„æµ‹å®Œæˆ")
            logger.info(f"è¯¦ç»†æŠ¥å‘Š: {report_file}")
            logger.info("="*60)
            
            return report_file
            
        except Exception as e:
            logger.error(f"è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def _save_evaluation_report(self) -> str:
        """ä¿å­˜è¯„æµ‹æŠ¥å‘Š"""
        report_file = os.path.join(
            self.output_dir,
            f"model_evaluation_report_{self.timestamp}.json"
        )
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file
    
    def _print_evaluation_summary(self):
        """æ‰“å°è¯„æµ‹æ‘˜è¦"""
        logger.info("\nè¯„æµ‹ç»“æœæ‘˜è¦:")
        logger.info("="*40)
        
        # Qwen3ç»“æœ
        qwen3_results = self.evaluation_results.get('qwen3_results', {})
        if qwen3_results and 'performance_summary' in qwen3_results:
            perf = qwen3_results['performance_summary']
            logger.info(f"\nğŸ“Š Qwen3æ¨¡å‹:")
            logger.info(f"  æˆåŠŸç‡: {perf.get('success_rate', 0):.2%}")
            logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {perf.get('avg_execution_time', 0):.2f}ç§’")
            logger.info(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {perf.get('avg_memory_usage', 0):.1f}MB")
        
        # SmolVLM2ç»“æœ
        smolvlm2_results = self.evaluation_results.get('smolvlm2_results', {})
        if smolvlm2_results and 'performance_summary' in smolvlm2_results:
            perf = smolvlm2_results['performance_summary']
            logger.info(f"\nğŸ“Š SmolVLM2æ¨¡å‹:")
            logger.info(f"  æˆåŠŸç‡: {perf.get('success_rate', 0):.2%}")
            logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {perf.get('avg_execution_time', 0):.2f}ç§’")
            logger.info(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {perf.get('avg_memory_usage', 0):.1f}MB")
            
            # å¤šæ¨¡æ€èƒ½åŠ›
            multimodal_tests = smolvlm2_results.get('multimodal_analysis', [])
            if multimodal_tests:
                logger.info(f"  å¤šæ¨¡æ€æµ‹è¯•: {len(multimodal_tests)}ä¸ª")
        
        # æ¯”è¾ƒç»“æœ
        comparison = self.evaluation_results.get('comparison', {})
        if comparison and 'recommendations' in comparison:
            logger.info(f"\nğŸ’¡ å»ºè®®:")
            for i, rec in enumerate(comparison['recommendations'][:3], 1):
                logger.info(f"  {i}. {rec}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Qwenå’ŒSmolVLM2æ¨¡å‹æ•ˆæœè¯„æµ‹è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--output-dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰')
    
    args = parser.parse_args()
    
    try:
        evaluator = ModelEvaluator(output_dir=args.output_dir)
        report_file = evaluator.run_evaluation()
        print(f"\nâœ… è¯„æµ‹å®Œæˆï¼è¯¦ç»†æŠ¥å‘Š: {report_file}")
        return 0
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())