"""
SmolVLM2 å¤šæ¨¡æ€æ¨¡å‹å®¢æˆ·ç«¯
åŸºäº transformers åº“å®ç°çš„ SmolVLM2 æ¨¡å‹æ¨ç†å®¢æˆ·ç«¯
æ”¯æŒå›¾åƒåˆ†æã€æ–‡æœ¬ç”Ÿæˆå’Œå¤è‘£é‰´å®šåŠŸèƒ½
"""

import os
import sys
import torch
import json
import logging
from PIL import Image
from typing import Union, Dict, Any, Optional, List
import numpy as np
from datetime import datetime

# ç‰ˆæœ¬æ¯”è¾ƒç›¸å…³å¯¼å…¥
try:
    from packaging import version
except ImportError:
    # å¦‚æœpackagingä¸å¯ç”¨ï¼Œä½¿ç”¨distutilsä½œä¸ºå¤‡é€‰
    try:
        from distutils.version import LooseVersion as version
    except ImportError:
        version = None

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
project_root = os.path.join(os.path.dirname(__file__), '..', '..')
sys.path.append(project_root)

# å°è¯•å¯¼å…¥loggeré…ç½®
try:
    from utils.logger_config import get_ai_logger
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„logger
    import logging
    def get_ai_logger(name):
        logger = logging.getLogger(name)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger

# é…ç½®æ—¥å¿—
logger = get_ai_logger('smolvlm2_client')

# æ£€æŸ¥ transformers åº“æ˜¯å¦å¯ç”¨
try:
    from transformers import (
        AutoProcessor, AutoModelForVision2Seq,
        AutoConfig, AutoTokenizer
    )
    TRANSFORMERS_AVAILABLE = True
    logger.info("transformers åº“å¯ç”¨")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    logger.error(f"transformers åº“ä¸å¯ç”¨: {e}")

# æ£€æŸ¥ GPU æ”¯æŒ
CUDA_AVAILABLE = torch.cuda.is_available()


class SmolVLM2Client:
    """SmolVLM2 å¤šæ¨¡æ€æ¨¡å‹å®¢æˆ·ç«¯
    
    ä½¿ç”¨ transformers åº“çš„ AutoModelForVision2Seq åŠ è½½å’Œæ¨ç† SmolVLM2 æ¨¡å‹
    æ”¯æŒå›¾åƒåˆ†æã€å¯¹è¯å’Œå¤è‘£é‰´å®šåŠŸèƒ½
    """
    
    def __init__(self, 
                 model_path: Optional[str] = None,
                 device: Optional[str] = None,
                 max_tokens: int = 512,
                 temperature: float = 0.7,
                 quantization_mode: Optional[str] = None):
        """
        åˆå§‹åŒ– SmolVLM2 å®¢æˆ·ç«¯
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            device: è®¾å¤‡ç±»å‹ï¼ˆcuda/cpu/autoï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
            quantization_mode: é‡åŒ–æ¨¡å¼ï¼ˆauto/4bit/8bit/noneï¼‰
            
        Raises:
            RuntimeError: å½“ transformers åº“ä¸å¯ç”¨æˆ–æ¨¡å‹åŠ è½½å¤±è´¥æ—¶
        """
        # æ£€æŸ¥ transformers åº“
        if not TRANSFORMERS_AVAILABLE:
            raise RuntimeError(
                "transformers åº“ä¸å¯ç”¨ï¼ŒSmolVLM2 å®¢æˆ·ç«¯éœ€è¦ transformers åº“æ‰èƒ½è¿è¡Œã€‚\n"
                "è¯·å®‰è£…ï¼špip install transformers>=4.46.0"
            )
        
        # ç¡®å®šæ¨¡å‹è·¯å¾„
        self.model_path = model_path or self._get_default_model_path()
        logger.info(f"ä½¿ç”¨æ¨¡å‹è·¯å¾„: {self.model_path}")
        
        # GPUè®¾å¤‡æ™ºèƒ½æ£€æµ‹å’Œé…ç½®
        if device == "auto" or device is None:
            if CUDA_AVAILABLE:
                self.device = "cuda"
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if gpu_count > 0 else 0
                logger.info(f"æ£€æµ‹åˆ°GPU: {gpu_name}, æ˜¾å­˜: {gpu_memory:.1f}GB")
                logger.info(f"å°†ä½¿ç”¨GPUåŠ é€Ÿ SmolVLM2 æ¨¡å‹æ¨ç†")
            else:
                self.device = "cpu"
                logger.info("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        else:
            self.device = device
            if device == "cuda" and not CUDA_AVAILABLE:
                logger.warning("æŒ‡å®šä½¿ç”¨GPUä½†æœªæ£€æµ‹åˆ°CUDAï¼Œå›é€€åˆ°CPUæ¨¡å¼")
                self.device = "cpu"
        
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.quantization_mode = quantization_mode  # æ–°å¢é‡åŒ–æ¨¡å¼å‚æ•°
        
        # éªŒè¯é‡åŒ–æ¨¡å¼å‚æ•°
        valid_quant_modes = [None, "auto", "4bit", "8bit", "none"]
        if self.quantization_mode not in valid_quant_modes:
            logger.warning(f"æ— æ•ˆçš„é‡åŒ–æ¨¡å¼: {self.quantization_mode}ï¼Œä½¿ç”¨å…¨é‡ç²¾åº¦åŠ è½½")
            self.quantization_mode = "none"
        
        # å¦‚æœæœªæŒ‡å®šé‡åŒ–æ¨¡å¼ï¼Œä½¿ç”¨noneä½œä¸ºé»˜è®¤å€¼ï¼ˆå…¨é‡åŠ è½½ï¼‰
        if self.quantization_mode is None or self.quantization_mode == "auto":
            self.quantization_mode = "none"
            logger.info("æœªæŒ‡å®šé‡åŒ–æ¨¡å¼ï¼Œä½¿ç”¨å…¨é‡ç²¾åº¦åŠ è½½")
        
        # åªæœ‰åœ¨éœ€è¦é‡åŒ–æ—¶æ‰æ£€æŸ¥bitsandbyteså…¼å®¹æ€§
        if self.quantization_mode in ["4bit", "8bit"]:
            # æ£€æŸ¥bitsandbytesç‰ˆæœ¬å…¼å®¹æ€§
            if not self._check_bitsandbytes_compatibility():
                logger.warning("æ£€æµ‹åˆ°bitsandbyteså…¼å®¹æ€§é—®é¢˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼å¹¶ç¦ç”¨é‡åŒ–")
                self.device = "cpu"
                self.quantization_mode = "none"
        else:
            logger.info("ä½¿ç”¨å…¨é‡ç²¾åº¦åŠ è½½ï¼Œè·³è¿‡bitsandbytesæ£€æŸ¥")
        
        if self.quantization_mode:
            logger.info(f"æŒ‡å®šé‡åŒ–æ¨¡å¼: {self.quantization_mode}")
        
        self.model = None
        self.processor = None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # é…ç½® CUDA æ˜¾å­˜ä¼˜åŒ–
        self._configure_cuda_memory()
    
    def _get_default_model_path(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹è·¯å¾„"""
        base_dir = os.path.dirname(__file__)  # models/client ç›®å½•
        models_dir = os.path.dirname(base_dir)  # models ç›®å½•
        
        # ä¼˜å…ˆæ£€æŸ¥ SmolVLM2 256M model (è½»é‡çº§æ¨¡å‹)
        small_model_path = os.path.join(models_dir, "smolvlm2_256M")
        if os.path.exists(small_model_path):
            # æ£€æŸ¥å…³é”®é…ç½®æ–‡ä»¶
            required_files = ['config.json', 'tokenizer_config.json', 'processor_config.json']
            missing_files = []
            for file in required_files:
                file_path = os.path.join(small_model_path, file)
                if not os.path.exists(file_path):
                    missing_files.append(file)
            
            if not missing_files:
                logger.info(f"æ‰¾åˆ° SmolVLM2 Small Model (256M): {small_model_path}")
                logger.info("ä½¿ç”¨è½»é‡çº§ SmolVLM2-256M æ¨¡å‹ï¼Œå†…å­˜å ç”¨æ›´å°‘")
                return small_model_path
            else:
                logger.warning(f"Small model è·¯å¾„ç¼ºå°‘æ–‡ä»¶: {missing_files}")
        
        # æ£€æŸ¥æ ‡å‡† SmolVLM2 æ¨¡å‹ç›®å½• (2.2Bå‚æ•°)
        default_model_path = os.path.join(models_dir, "SmolVLM2_model")
        if os.path.exists(default_model_path):
            # æ£€æŸ¥å…³é”®é…ç½®æ–‡ä»¶
            required_files = ['config.json', 'tokenizer_config.json', 'processor_config.json']
            missing_files = []
            for file in required_files:
                file_path = os.path.join(default_model_path, file)
                if not os.path.exists(file_path):
                    missing_files.append(file)
            
            if not missing_files:
                logger.info(f"æ‰¾åˆ°æ ‡å‡† SmolVLM2 æ¨¡å‹ (2.2B): {default_model_path}")
                return default_model_path
            else:
                logger.warning(f"æ ‡å‡†æ¨¡å‹è·¯å¾„ç¼ºå°‘æ–‡ä»¶: {missing_files}")
        
        # ä¼˜å…ˆä½¿ç”¨ ModelScope æ¨¡å‹ï¼ˆå›½å†…è®¿é—®æ›´å¿«ï¼‰
        logger.info("æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†ä¼˜å…ˆä» ModelScope ä¸‹è½½")
        return "HuggingFaceTB/SmolVLM2-2.2B-Base"
    
    def _configure_cuda_memory(self):
        """é…ç½® CUDA æ˜¾å­˜ä¼˜åŒ–å‚æ•°"""
        # è®¾ç½®PyTorchå†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        
        if CUDA_AVAILABLE and self.device == "cuda":
            try:
                # æ¸…ç†æ˜¾å­˜ç¼“å­˜
                torch.cuda.empty_cache()
                
                # è·å–GPUä¿¡æ¯
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
                gpu_free_gb = gpu_free_memory / 1024**3
                
                logger.info(f"CUDAæ˜¾å­˜ä¼˜åŒ–å·²å¯ç”¨: {gpu_memory_gb:.1f}GB GPU, å¯ç”¨: {gpu_free_gb:.1f}GB")
                
                # æ ¹æ®æ˜¾å­˜å¤§å°é…ç½®ä¼˜åŒ–ç­–ç•¥
                if gpu_memory_gb < 6:
                    logger.warning(f"æ˜¾å­˜è¾ƒå°({gpu_memory_gb:.1f}GB)ï¼Œå°†å¯ç”¨æ¿€è¿›å†…å­˜ä¼˜åŒ–")
                    # å¯ç”¨æ˜¾å­˜åˆ†ç‰‡ä»¥é¿å…å†…å­˜ç¢ç‰‡
                    torch.cuda.set_per_process_memory_fraction(0.8)
                elif gpu_memory_gb < 8:
                    logger.info(f"ä¸­ç­‰æ˜¾å­˜({gpu_memory_gb:.1f}GB)ï¼Œå¯ç”¨æ ‡å‡†å†…å­˜ä¼˜åŒ–")
                    torch.cuda.set_per_process_memory_fraction(0.9)
                else:
                    logger.info(f"å……è¶³æ˜¾å­˜({gpu_memory_gb:.1f}GB)ï¼Œä½¿ç”¨é»˜è®¤å†…å­˜è®¾ç½®")
                
            except Exception as e:
                logger.warning(f"CUDAæ˜¾å­˜ä¼˜åŒ–é…ç½®å¤±è´¥: {e}")
        
        # é…ç½®é€šç”¨å†…å­˜ä¼˜åŒ–
        try:
            import gc
            gc.collect()
            
            # è®¾ç½®PyTorchçº¿ç¨‹æ•°ï¼ˆé¿å…CPUè¿‡è½½ï¼‰
            optimal_threads = min(4, max(1, torch.get_num_threads() // 2))
            torch.set_num_threads(optimal_threads)
            logger.info(f"è®¾ç½®PyTorchçº¿ç¨‹æ•°: {optimal_threads}")
            
            # å¯ç”¨å†…å­˜æ˜ å°„ä»¥å‡å°‘å†…å­˜å ç”¨
            torch.backends.cudnn.benchmark = False
            torch.backends.cudnn.deterministic = True
            
        except Exception as e:
            logger.warning(f"é€šç”¨å†…å­˜ä¼˜åŒ–é…ç½®å¤±è´¥: {e}")
    
    def _init_model(self):
        """åˆå§‹åŒ– SmolVLM2 æ¨¡å‹ï¼ˆæŒ‰ç…§ModelScopeæ ‡å‡†å®ç°ï¼‰"""
        try:
            logger.info(f"æ­£åœ¨ä½¿ç”¨ transformers AutoModelForVision2Seq åŠ è½½ SmolVLM2 æ¨¡å‹: {self.model_path}")
            
            # æ£€æŸ¥æ¨¡å‹é…ç½®
            if os.path.exists(self.model_path):
                config_path = os.path.join(self.model_path, 'config.json')
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    model_type = config.get('model_type', 'unknown')
                    architectures = config.get('architectures', [])
                    logger.info(f"æ¨¡å‹ç±»å‹: {model_type}, æ¶æ„: {architectures}")
            
            # æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # è·å–æœ€ä¼˜åŠ è½½é…ç½®
            load_kwargs = self._get_optimal_load_config()
            
            # åŠ è½½å¤„ç†å™¨ï¼ˆè½»é‡çº§ï¼Œä¼˜å…ˆåŠ è½½ï¼‰
            logger.info("æ­£åœ¨åŠ è½½ AutoProcessor...")
            import warnings
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", message=".*special tokens.*")
                warnings.filterwarnings("ignore", message=".*image processor.*")
                warnings.filterwarnings("ignore", message=".*torch_dtype.*")
                
                # æŒ‰ç…§ModelScopeæ ‡å‡†åŠ è½½æ–¹å¼
                self.processor = AutoProcessor.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.bfloat16,
                    cache_dir=None,  # ä½¿ç”¨é»˜è®¤ç¼“å­˜ç›®å½•
                )
            
            # å†æ¬¡æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # åŠ è½½æ¨¡å‹ï¼ˆå†…å­˜å¯†é›†å‹æ“ä½œï¼‰
            logger.info("æ­£åœ¨åŠ è½½ AutoModelForVision2Seq...")
            logger.info(f"åŠ è½½å‚æ•°: {load_kwargs}")
            
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")
                
                # æŒ‰ç…§ModelScopeæ ‡å‡†åŠ è½½æ–¹å¼
                self.model = AutoModelForVision2Seq.from_pretrained(
                    self.model_path,
                    cache_dir=None,  # ä½¿ç”¨é»˜è®¤ç¼“å­˜ç›®å½•
                    **load_kwargs
                )
            
            # CPUæ¨¡å¼éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device == "cpu" and "device_map" not in load_kwargs:
                logger.info("å°†æ¨¡å‹ç§»åŠ¨åˆ°CPU...")
                self.model = self.model.to(self.device)
            
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # æœ€ç»ˆå†…å­˜æ¸…ç†
            self._cleanup_memory()
            
            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
            actual_dtype = next(self.model.parameters()).dtype if self.model else "unknown"
            logger.info(f"âœ… æˆåŠŸåŠ è½½ SmolVLM2 æ¨¡å‹")
            logger.info(f"ğŸ“ æ¨¡å‹ç±»å‹: SmolVLM2-2.2B")
            logger.info(f"ğŸ“ è®¾å¤‡: {self.device}")
            logger.info(f"ğŸ“ æ•°æ®ç±»å‹: {actual_dtype}")
            logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {getattr(self.processor.tokenizer, 'model_max_length', 'unknown')}")
            
            # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
            if CUDA_AVAILABLE and self.device == "cuda":
                allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
                cached_memory = torch.cuda.memory_reserved(0) / 1024**3
                logger.info(f"ğŸ“ GPUå†…å­˜ä½¿ç”¨: å·²åˆ†é…{allocated_memory:.1f}GB, ç¼“å­˜{cached_memory:.1f}GB")
            
        except Exception as e:
            error_msg = f"ä½¿ç”¨ transformers åŠ è½½ SmolVLM2 æ¨¡å‹å¤±è´¥: {e}"
            logger.error(error_msg)
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            
            # æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆ
            if "memory" in str(e).lower() or "out of memory" in str(e).lower():
                error_msg += "\n\nğŸ’¡ å†…å­˜ä¸è¶³è§£å†³æ–¹æ¡ˆ:"
                error_msg += "\n1. ä½¿ç”¨ CPU æ¨¡å¼ï¼š--device cpu"
                error_msg += "\n2. å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„åº”ç”¨"
                error_msg += "\n3. å¢åŠ ç³»ç»Ÿè™šæ‹Ÿå†…å­˜å¤§å°"
                error_msg += "\n4. å°è¯•é‡å¯ç³»ç»Ÿé‡Šæ”¾å†…å­˜"
            elif "load_in_8bit" in str(e).lower() or "bitsandbytes" in str(e).lower() or "SCB" in str(e):
                error_msg += "\n\nğŸ’¡ é‡åŒ–åº“é—®é¢˜è§£å†³æ–¹æ¡ˆ:"
                if "SCB" in str(e):
                    error_msg += "\nâš ï¸  æ£€æµ‹åˆ°SCBå±æ€§é”™è¯¯ï¼Œè¿™æ˜¯bitsandbytesç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜"
                    error_msg += "\n1. å¸è½½å½“å‰ç‰ˆæœ¬: pip uninstall bitsandbytes"
                    error_msg += "\n2. å®‰è£…å…¼å®¹ç‰ˆæœ¬: pip install bitsandbytes>=0.43.0"
                    error_msg += "\n3. é‡å¯Pythonç¯å¢ƒåé‡è¯•"
                    error_msg += "\n4. æˆ–ä½¿ç”¨ CPU æ¨¡å¼ï¼š--device cpu"
                    error_msg += "\n5. æˆ–ç¦ç”¨é‡åŒ–ï¼š--quantization none"
                else:
                    error_msg += "\n1. å®‰è£… bitsandbytes: pip install bitsandbytes>=0.43.0"
                    error_msg += "\n2. æˆ–ä½¿ç”¨ CPU æ¨¡å¼ï¼š--device cpu"
            elif "flash_attention" in str(e).lower():
                error_msg += "\n\nğŸ’¡ FlashAttention é—®é¢˜è§£å†³æ–¹æ¡ˆ:"
                error_msg += "\n1. å®‰è£… flash-attn: pip install flash-attn"
                error_msg += "\n2. æˆ–å°†åœ¨ä»£ç ä¸­ç¦ç”¨ flash_attention"
            elif "transformers" in str(e).lower():
                error_msg += "\n\nğŸ’¡ Transformers é—®é¢˜è§£å†³æ–¹æ¡ˆ:"
                error_msg += "\n1. æ›´æ–° transformers: pip install transformers>=4.47.0 --upgrade"
                error_msg += "\n2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"
                error_msg += "\n3. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆå¦‚éœ€ä¸‹è½½æ¨¡å‹ï¼‰"
            
            # æ¸…ç†å†…å­˜åå†æŠ›å‡ºå¼‚å¸¸
            self._cleanup_memory()
            raise RuntimeError(error_msg)
            
    def _get_optimal_load_config(self) -> Dict[str, Any]:
        """æ ¹æ®ç³»ç»Ÿèµ„æºè·å–æœ€ä¼˜åŠ è½½é…ç½®"""
        load_kwargs = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
        }
        
        if self.device == "cuda" and CUDA_AVAILABLE:
            try:
                # è·å–GPUæ˜¾å­˜ä¿¡æ¯
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
                gpu_free_gb = gpu_free_memory / 1024**3
                
                logger.info(f"æ˜¾å­˜çŠ¶æ€: æ€»é‡{gpu_memory_gb:.1f}GB, å¯ç”¨{gpu_free_gb:.1f}GB")
                
                # åªæœ‰åœ¨éœ€è¦é‡åŒ–æ—¶æ‰æ£€æŸ¥é‡åŒ–åº“æ”¯æŒ
                bitsandbytes_available = False
                if self.quantization_mode in ["4bit", "8bit"]:
                    bitsandbytes_available = self._check_bitsandbytes()
                    # å¦‚æœæ£€æµ‹åˆ°SCBé—®é¢˜ï¼Œç¦ç”¨é‡åŒ–
                    if not self._check_bitsandbytes_compatibility():
                        logger.warning("æ£€æµ‹åˆ°bitsandbyteså…¼å®¹æ€§é—®é¢˜ï¼Œç¦ç”¨é‡åŒ–")
                        bitsandbytes_available = False
                
                # å¤„ç†å¼ºåˆ¶é‡åŒ–æ¨¡å¼
                if self.quantization_mode == "4bit":
                    if bitsandbytes_available:
                        logger.info("å¼ºåˆ¶ä½¿ç”¨4bité‡åŒ–æ¨¡å¼")
                        quantization_config = self._get_4bit_config()
                        if quantization_config:
                            load_kwargs.update({
                                "dtype": torch.float16,
                                "device_map": "auto",
                                "quantization_config": quantization_config,
                                "max_memory": {0: f"{int(gpu_free_gb * 0.7)}GB", "cpu": "6GB"}
                            })
                            return load_kwargs
                    logger.error("4bité‡åŒ–ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install bitsandbytes")
                    raise RuntimeError("4bité‡åŒ–ä¸å¯ç”¨")
                elif self.quantization_mode == "8bit":
                    if bitsandbytes_available:
                        logger.info("å¼ºåˆ¶ä½¿ç”¨8bité‡åŒ–æ¨¡å¼")
                        quantization_config = self._get_8bit_config()
                        if quantization_config:
                            load_kwargs.update({
                                "dtype": torch.float16,
                                "device_map": "auto",
                                "quantization_config": quantization_config,
                                "max_memory": {0: f"{int(gpu_free_gb * 0.8)}GB", "cpu": "4GB"}
                            })
                            return load_kwargs
                    logger.error("8bité‡åŒ–ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install bitsandbytes")
                    raise RuntimeError("8bité‡åŒ–ä¸å¯ç”¨")
                elif self.quantization_mode == "none":
                    logger.info("å¼ºåˆ¶ç¦ç”¨é‡åŒ–ï¼Œä½¿ç”¨åŸå§‹ç²¾åº¦")
                    load_kwargs.update({
                        "dtype": torch.bfloat16 if gpu_memory_gb >= 12 else torch.float16,
                        "device_map": "auto"
                    })
                    return load_kwargs
                
                # æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹©åŠ è½½ç­–ç•¥ï¼ˆå…¨é‡ç²¾åº¦åŠ è½½ï¼Œæ— é‡åŒ–ï¼‰
                if gpu_memory_gb < 4:
                    # æå°æ˜¾å­˜: å¼ºåˆ¶CPUæ¨¡å¼
                    logger.warning("æ˜¾å­˜ä¸¥é‡ä¸è¶³(<4GB)ï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
                    self.device = "cpu"
                    load_kwargs.update({
                        "dtype": torch.float32,
                        "device_map": "cpu",
                        "use_safetensors": True
                    })
                elif gpu_memory_gb < 6:
                    # å°æ˜¾å­˜: ä½¿ç”¨float16ç²¾åº¦
                    logger.info("æ˜¾å­˜è¾ƒå°(<6GB)ï¼Œä½¿ç”¨float16ç²¾åº¦å…¨é‡åŠ è½½")
                    load_kwargs.update({
                        "dtype": torch.float16,
                        "device_map": "auto",
                        "max_memory": {0: f"{int(gpu_free_gb * 0.8)}GB", "cpu": "8GB"}
                    })
                elif gpu_memory_gb < 8:
                    # ä¸­å°æ˜¾å­˜: ä½¿ç”¨float16ç²¾åº¦
                    logger.info("ä¸­å°æ˜¾å­˜(6-8GB)ï¼Œä½¿ç”¨float16ç²¾åº¦å…¨é‡åŠ è½½")
                    load_kwargs.update({
                        "dtype": torch.float16,
                        "device_map": "auto",
                        "max_memory": {0: f"{int(gpu_free_gb * 0.85)}GB", "cpu": "4GB"}
                    })
                elif gpu_memory_gb < 12:
                    # ä¸­ç­‰æ˜¾å­˜: ä½¿ç”¨float16ç²¾åº¦
                    logger.info("ä¸­ç­‰æ˜¾å­˜(8-12GB)ï¼Œä½¿ç”¨float16ç²¾åº¦å…¨é‡åŠ è½½")
                    load_kwargs.update({
                        "dtype": torch.float16,
                        "device_map": "auto",
                        "max_memory": {0: f"{int(gpu_free_gb * 0.9)}GB", "cpu": "2GB"}
                    })
                else:
                    # å……è¶³æ˜¾å­˜: ä½¿ç”¨bfloat16ç²¾åº¦
                    logger.info("å……è¶³æ˜¾å­˜(>=12GB)ï¼Œä½¿ç”¨bfloat16ç²¾åº¦å…¨é‡åŠ è½½")
                    load_kwargs.update({
                        "dtype": torch.bfloat16,
                        "device_map": "auto"
                    })
                
                # flash_attention æ˜¯å¯é€‰çš„ï¼ˆä»…åœ¨éé‡åŒ–æ¨¡å¼ä¸‹ï¼‰
                if not load_kwargs.get("quantization_config"):
                    try:
                        import flash_attn
                        load_kwargs["attn_implementation"] = "flash_attention_2"
                        logger.info("å¯ç”¨ FlashAttention2 åŠ é€Ÿ")
                    except ImportError:
                        logger.info("FlashAttention2 ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›æœºåˆ¶")
                    
            except Exception as e:
                logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤GPUé…ç½®")
                load_kwargs.update({
                    "dtype": torch.float16,
                    "device_map": "auto"
                })
        else:
            # CPUæ¨¡å¼ä¼˜åŒ–
            logger.info("ä½¿ç”¨CPUæ¨¡å¼ï¼Œå¯ç”¨å†…å­˜ä¼˜åŒ–")
            load_kwargs.update({
                "dtype": torch.float32,
                "device_map": "cpu",
                "use_safetensors": True
            })
        
        return load_kwargs
        
    def _get_4bit_config(self):
        """è·å–4bité‡åŒ–é…ç½®ï¼ˆé’ˆå¯¹SmolVLM2ä¼˜åŒ–ï¼‰"""
        try:
            from transformers import BitsAndBytesConfig
            # é’ˆå¯¹SmolVLM2çš„4bité‡åŒ–é…ç½®ï¼Œå¢å¼ºå…¼å®¹æ€§
            # æ·»åŠ æ›´å¤šå…¼å®¹æ€§é…ç½®ä»¥è§£å†³SCBå±æ€§é”™è¯¯
            config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                # å¢åŠ ç¨³å®šæ€§é€‰é¡¹
                bnb_4bit_quant_storage=torch.uint8,
                # æ·»åŠ æ›´å¤šå…¼å®¹æ€§é€‰é¡¹
                bnb_4bit_skip_modules=["lm_head"],
            )
            logger.info("âœ… 4bité‡åŒ–é…ç½®åˆ›å»ºæˆåŠŸ")
            return config
        except ImportError:
            logger.error("BitsAndBytesConfig ä¸å¯ç”¨ï¼Œè¯·æ›´æ–° transformers")
            return None
        except Exception as e:
            error_str = str(e)
            if "SCB" in error_str:
                logger.error(f"4bité‡åŒ–é…ç½®åˆ›å»ºå¤±è´¥ï¼ˆSCBé”™è¯¯ï¼‰: {e}")
                logger.error("è¿™æ˜¯bitsandbytesç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·æ›´æ–°åˆ°>=0.41.0")
            else:
                logger.error(f"4bité‡åŒ–é…ç½®åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    def _get_8bit_config(self):
        """è·å–8bité‡åŒ–é…ç½®ï¼ˆæ¨èç”¨äºSmolVLM2ï¼‰"""
        try:
            from transformers import BitsAndBytesConfig
            # 8bité‡åŒ–æ˜¯SmolVLM2çš„æ¨èé…ç½®ï¼Œå…¼å®¹æ€§æ›´å¥½
            # æ·»åŠ æ›´å¤šå…¼å®¹æ€§é…ç½®ä»¥è§£å†³SCBå±æ€§é”™è¯¯
            config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.float16,
                llm_int8_enable_fp32_cpu_offload=False,  # ç¦ç”¨CPUå¸è½½æé«˜ç¨³å®šæ€§
                llm_int8_has_fp16_weight=True,
                # æ·»åŠ æ›´å¤šå…¼å®¹æ€§é€‰é¡¹
                llm_int8_skip_modules=["lm_head"],
                # è§£å†³SCBå±æ€§é”™è¯¯çš„é¢å¤–é…ç½®
                llm_int8_threshold=6.0,
            )
            logger.info("âœ… 8bité‡åŒ–é…ç½®åˆ›å»ºæˆåŠŸ")
            return config
        except ImportError:
            logger.error("BitsAndBytesConfig ä¸å¯ç”¨ï¼Œè¯·æ›´æ–° transformers")
            return None
        except Exception as e:
            error_str = str(e)
            if "SCB" in error_str:
                logger.error(f"8bité‡åŒ–é…ç½®åˆ›å»ºå¤±è´¥ï¼ˆSCBé”™è¯¯ï¼‰: {e}")
                logger.error("è¿™æ˜¯bitsandbytesç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·æ›´æ–°åˆ°>=0.41.0")
            else:
                logger.error(f"8bité‡åŒ–é…ç½®åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    def _check_bitsandbytes_compatibility(self) -> bool:
        """æ£€æŸ¥ bitsandbytes é‡åŒ–åº“æ˜¯å¦å¯ç”¨ä¸”å…¼å®¹"""
        try:
            import bitsandbytes as bnb
            bnb_version = bnb.__version__
            logger.info(f"bitsandbytes é‡åŒ–åº“å¯ç”¨ï¼Œç‰ˆæœ¬: {bnb_version}")
            
            # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
            import torch
            
            torch_version = torch.__version__.split('+')[0]  # å»é™¤CUDAåç¼€
            
            # æ£€æŸ¥PyTorchä¸bitsandbytesçš„å…¼å®¹æ€§
            if version is not None:
                try:
                    if version.parse(torch_version) >= version.parse("2.0.0"):
                        if version.parse(bnb_version) < version.parse("0.43.0"):
                            logger.warning(f"bitsandbytesç‰ˆæœ¬{bnb_version}å¯èƒ½ä¸PyTorch {torch_version}ä¸å…¼å®¹")
                            logger.warning("å»ºè®®æ›´æ–°: pip install bitsandbytes>=0.43.0 --upgrade")
                            return False
                except Exception as ver_e:
                    logger.warning(f"ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {ver_e}ï¼Œç»§ç»­è¿›è¡ŒåŠŸèƒ½æ€§æµ‹è¯•")
            else:
                logger.warning("æ— æ³•è¿›è¡Œç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥ï¼Œç»§ç»­è¿›è¡ŒåŠŸèƒ½æ€§æµ‹è¯•")
            
            # è¿›è¡Œç®€å•çš„åŠŸèƒ½æ€§æµ‹è¯•
            try:
                from transformers import BitsAndBytesConfig
                # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®æµ‹è¯•å…¼å®¹æ€§
                test_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
                logger.info("âœ… bitsandbytesåŠŸèƒ½æ€§æµ‹è¯•é€šè¿‡")
                return True
                
            except Exception as test_e:
                logger.error(f"âŒ bitsandbytesåŠŸèƒ½æ€§æµ‹è¯•å¤±è´¥: {test_e}")
                if "SCB" in str(test_e):
                    logger.error("æ£€æµ‹åˆ°SCBå±æ€§é”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
                    logger.error("è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
                    logger.error("1. pip uninstall bitsandbytes")
                    logger.error("2. pip install bitsandbytes>=0.43.0")
                    logger.error("3. æˆ–ä½¿ç”¨CPUæ¨¡å¼: --device cpu")
                return False
                
        except ImportError:
            logger.warning("bitsandbytes é‡åŒ–åº“ä¸å¯ç”¨ï¼Œæ— æ³•4bit/8bité‡åŒ–")
            return False
        except Exception as e:
            logger.error(f"bitsandbytesæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _check_bitsandbytes(self) -> bool:
        """æ£€æŸ¥ bitsandbytes é‡åŒ–åº“æ˜¯å¦å¯ç”¨"""
        try:
            import bitsandbytes as bnb
            logger.info(f"bitsandbytes é‡åŒ–åº“å¯ç”¨ï¼Œç‰ˆæœ¬: {bnb.__version__}")
            return True
        except ImportError:
            logger.warning("bitsandbytes é‡åŒ–åº“ä¸å¯ç”¨ï¼Œæ— æ³•4bit/8bité‡åŒ–")
            return False
        except Exception as e:
            logger.error(f"bitsandbytesæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    
    def _preprocess_image_standard(self, image: Union[str, Image.Image, np.ndarray]) -> Image.Image:
        """æŒ‰ç…§ModelScopeæ ‡å‡†é¢„å¤„ç†å›¾åƒ"""
        # å¤„ç†å›¾åƒè¾“å…¥
        if isinstance(image, str):
            pil_image = Image.open(image).convert('RGB')
            logger.info(f"ä»æ–‡ä»¶åŠ è½½å›¾åƒ: {image}")
        elif isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image).convert('RGB')
        else:
            pil_image = image.convert('RGB')
        
        # æŒ‰ç…§ModelScopeæ¨èå°ºå¯¸ï¼š384x384
        max_size = 384
        if max(pil_image.size) > max_size:
            ratio = max_size / max(pil_image.size)
            new_size = tuple(int(dim * ratio) for dim in pil_image.size)
            pil_image = pil_image.resize(new_size, Image.Resampling.LANCZOS)
            logger.info(f"å›¾åƒå·²ç¼©æ”¾è‡³: {new_size}")
        
        return pil_image
    
    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜å’Œæ˜¾å­˜"""
        try:
            import gc
            gc.collect()
            
            if CUDA_AVAILABLE and self.device == "cuda":
                torch.cuda.empty_cache()
                # å¼ºåˆ¶åŒæ­¥æ˜¾å­˜æ“ä½œ
                torch.cuda.synchronize()
                
        except Exception as e:
            logger.warning(f"å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def analyze_image(self, 
                     image: Union[str, Image.Image, np.ndarray],
                     prompt: str = "è¯·è¯¦ç»†åˆ†æè¿™å¼ å¤è‘£æ–‡ç‰©å›¾ç‰‡ï¼ŒåŒ…æ‹¬æ–‡ç‰©ç±»å‹ã€å¹´ä»£ã€æè´¨ã€å·¥è‰ºç‰¹ç‚¹ã€çœŸä¼ªè¯„ä¼°ã€ä¿å­˜çŠ¶å†µå’Œå†å²ä»·å€¼ã€‚è¯·ç¡®ä¿å›ç­”å®Œå…¨ä½¿ç”¨ä¸­æ–‡ï¼Œä¸è¦ä½¿ç”¨è‹±æ–‡ã€‚") -> str:
        """
        åˆ†æå›¾åƒå¹¶ç”Ÿæˆæè¿°
        
        Args:
            image: è¾“å…¥å›¾åƒ
            prompt: åˆ†ææç¤ºè¯
            
        Returns:
            åˆ†æç»“æœæ–‡æœ¬
            
        Raises:
            RuntimeError: å½“æ¨¡å‹æœªæ­£ç¡®åŠ è½½æ—¶
        """
        if not self.model or not self.processor:
            raise RuntimeError("æ¨¡å‹æˆ–å¤„ç†å™¨æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œå›¾åƒåˆ†æ")
        
        return self._analyze_with_model(image, prompt)
    
    def analyze_image_stream(self, 
                           image: Union[str, Image.Image, np.ndarray],
                           prompt: str = "è¯·è¯¦ç»†åˆ†æè¿™å¼ å¤è‘£æ–‡ç‰©å›¾ç‰‡ï¼ŒåŒ…æ‹¬æ–‡ç‰©ç±»å‹ã€å¹´ä»£ã€æè´¨ã€å·¥è‰ºç‰¹ç‚¹ã€çœŸä¼ªè¯„ä¼°ã€ä¿å­˜çŠ¶å†µå’Œå†å²ä»·å€¼ã€‚è¯·ç¡®ä¿å›ç­”å®Œå…¨ä½¿ç”¨ä¸­æ–‡ï¼Œä¸è¦ä½¿ç”¨è‹±æ–‡ã€‚"):
        """
        æµå¼åˆ†æå›¾åƒå¹¶ç”Ÿæˆæè¿°
        
        Args:
            image: è¾“å…¥å›¾åƒ
            prompt: åˆ†ææç¤ºè¯
            
        Yields:
            åˆ†æç»“æœæ–‡æœ¬ç‰‡æ®µ
            
        Raises:
            RuntimeError: å½“æ¨¡å‹æœªæ­£ç¡®åŠ è½½æ—¶
        """
        if not self.model or not self.processor:
            raise RuntimeError("æ¨¡å‹æˆ–å¤„ç†å™¨æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œå›¾åƒåˆ†æ")
        
        yield from self._analyze_with_model_stream(image, prompt)
    
    def _analyze_with_model(self, image: Union[str, Image.Image, np.ndarray], prompt: str) -> str:
        """ä½¿ç”¨ SmolVLM2 æ¨¡å‹åˆ†æå›¾åƒ"""
        try:
            # æ¨ç†å‰æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # æŒ‰ç…§ModelScopeæ ‡å‡†é¢„å¤„ç†å›¾åƒ
            pil_image = self._preprocess_image_standard(image)
            
            # æŒ‰ç…§ModelScopeæ ‡å‡†æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_input = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True,
                tokenize=False
            )
            
            # æŒ‰ç…§ModelScopeæ ‡å‡†å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_input,
                images=pil_image,
                return_tensors="pt",
                padding=True,
                truncation=False
            ).to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": min(self.max_tokens, 512),
                "temperature": self.temperature,
                "do_sample": True if self.temperature > 0 else False,
                "pad_token_id": self.processor.tokenizer.eos_token_id,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
                "use_cache": True,
                "repetition_penalty": 1.1,
            }
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                if CUDA_AVAILABLE and self.device == "cuda":
                    with torch.amp.autocast('cuda', dtype=torch.float16):
                        generated_ids = self.model.generate(**inputs, **generation_config)
                else:
                    generated_ids = self.model.generate(**inputs, **generation_config)
            
            # æ¸…ç†è¾“å…¥
            del inputs
            self._cleanup_memory()
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = self.processor.decode(
                generated_ids[0], 
                skip_special_tokens=True
            )
            
            # æ¸…ç†ç”Ÿæˆçš„ids
            del generated_ids
            self._cleanup_memory()
            
            # æå–å›å¤éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥promptï¼‰
            if "assistant" in generated_text.lower():
                parts = generated_text.split("assistant")
                if len(parts) > 1:
                    response = parts[-1].strip()
                    return response if response else "æ— æ³•ç”Ÿæˆæœ‰æ•ˆå›å¤"
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°assistantæ ‡è®°ï¼Œå°è¯•å…¶ä»–æ–¹å¼æå–
            response = generated_text.replace(text_input, "").strip()
            return response if response else "æ— æ³•ç”Ÿæˆæœ‰æ•ˆå›å¤"
            
        except Exception as e:
            logger.error(f"SmolVLM2 å›¾åƒåˆ†æå¤±è´¥: {e}")
            self._cleanup_memory()
            return f"åˆ†æå¤±è´¥: {str(e)}"
    
    def _analyze_with_model_stream(self, image: Union[str, Image.Image, np.ndarray], prompt: str):
        """ä½¿ç”¨ SmolVLM2 æ¨¡å‹æµå¼åˆ†æå›¾åƒ"""
        try:
            # æ¨ç†å‰æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # æŒ‰ç…§ModelScopeæ ‡å‡†é¢„å¤„ç†å›¾åƒ
            pil_image = self._preprocess_image_standard(image)
            
            # æŒ‰ç…§ModelScopeæ ‡å‡†æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_input = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True,
                tokenize=False
            )
            
            # æŒ‰ç…§ModelScopeæ ‡å‡†å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_input,
                images=pil_image,
                return_tensors="pt",
                padding=True,
                truncation=False
            ).to(self.device)
            
            # æµå¼ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": min(self.max_tokens, 512),
                "temperature": self.temperature,
                "do_sample": True if self.temperature > 0 else False,
                "pad_token_id": self.processor.tokenizer.eos_token_id,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
                "use_cache": True,
                "repetition_penalty": 1.1,
            }
            
            # æµå¼ç”Ÿæˆ
            with torch.no_grad():
                if CUDA_AVAILABLE and self.device == "cuda":
                    with torch.amp.autocast('cuda', dtype=torch.float16):
                        # ä½¿ç”¨æµå¼ç”Ÿæˆ
                        for new_token_id in self._generate_stream(**inputs, **generation_config):
                            if new_token_id is not None:
                                token_text = self.processor.tokenizer.decode([new_token_id], skip_special_tokens=True)
                                yield token_text
                else:
                    for new_token_id in self._generate_stream(**inputs, **generation_config):
                        if new_token_id is not None:
                            token_text = self.processor.tokenizer.decode([new_token_id], skip_special_tokens=True)
                            yield token_text
            
            # æ¸…ç†å†…å­˜
            del inputs
            self._cleanup_memory()
            
        except Exception as e:
            logger.error(f"SmolVLM2 æµå¼åˆ†æå¤±è´¥: {e}")
            self._cleanup_memory()
            yield f"é”™è¯¯: {str(e)}"
    
    def _generate_stream(self, **kwargs):
        """æµå¼ç”Ÿæˆtoken"""
        try:
            # è·å–è¾“å…¥é•¿åº¦
            input_length = kwargs['input_ids'].shape[1]
            
            # é€æ­¥ç”Ÿæˆ
            for step in range(kwargs.get('max_new_tokens', 512)):
                # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                with torch.no_grad():
                    outputs = self.model(**{k: v for k, v in kwargs.items() if k not in ['max_new_tokens']})
                    next_token_logits = outputs.logits[0, -1, :]
                    
                    # åº”ç”¨æ¸©åº¦
                    if kwargs.get('temperature', 1.0) != 1.0:
                        next_token_logits = next_token_logits / kwargs['temperature']
                    
                    # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                    if kwargs.get('do_sample', False):
                        probs = torch.softmax(next_token_logits, dim=-1)
                        next_token = torch.multinomial(probs, num_samples=1)
                    else:
                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                    
                    # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                    if next_token.item() == kwargs.get('eos_token_id'):
                        break
                    
                    # æ›´æ–°è¾“å…¥
                    kwargs['input_ids'] = torch.cat([kwargs['input_ids'], next_token.unsqueeze(0)], dim=1)
                    if 'attention_mask' in kwargs:
                        kwargs['attention_mask'] = torch.cat([
                            kwargs['attention_mask'], 
                            torch.ones((1, 1), device=kwargs['attention_mask'].device)
                        ], dim=1)
                    
                    yield next_token.item()
                    
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            yield None
    
    def generate_appraisal_report(self,
                                image: Union[str, Image.Image, np.ndarray],
                                user_query: str = "") -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤è‘£é‰´å®šæŠ¥å‘Š
        
        Args:
            image: å¤è‘£å›¾åƒ
            user_query: ç”¨æˆ·ç‰¹åˆ«å…³æ³¨çš„é—®é¢˜
            
        Returns:
            é‰´å®šæŠ¥å‘Šå­—å…¸
        """
        try:
            # æ„å»ºä¸“ä¸šçš„å¤è‘£é‰´å®šæç¤ºè¯
            base_prompt = """ä½œä¸ºä¸€ä½èµ„æ·±çš„å¤è‘£é‰´å®šä¸“å®¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿å¯¹è¿™ä»¶æ–‡ç‰©è¿›è¡Œè¯¦ç»†çš„ä¸“ä¸šé‰´èµåˆ†æï¼š

**ä¸€ã€è—å“åŸºç¡€ä¿¡æ¯ï¼ˆå…³é”®ä¿¡æ¯åˆ†ç±»ï¼‰**
**è—å“æ ¸å¿ƒå±æ€§**
- è—å“åç§°ï¼šã€ç²¾å‡†æ ‡æ³¨å“ç±»ã€å¹´ä»£ï¼ˆè‹¥åˆæ­¥åˆ¤æ–­ï¼‰ã€æ ¸å¿ƒç‰¹å¾ã€‘
- æè´¨ç±»å‹ï¼šã€æ˜ç¡®æè´¨åŠç»†åˆ†å“ç±»ï¼Œå¦‚é™¶ç“·ï¼ˆé’èŠ±ç“·/é’ç“·ï¼‰ã€é‡‘å±ï¼ˆé»„é“œ/ç™½é“¶ï¼‰ã€ç‰çŸ³ï¼ˆå’Œç”°ç‰/ç¿¡ç¿ ï¼‰ç­‰ã€‘
- è§„æ ¼å‚æ•°ï¼šã€å°½å¯èƒ½ä¼°ç®—å°ºå¯¸è§„æ ¼ã€‘
- å¤–è§‚ç‰¹å¾ï¼šã€ç®€è¦æè¿°æ•´ä½“çŠ¶æ€å’Œä¿å­˜æƒ…å†µã€‘

**äºŒã€è—å“çœŸä¼ªåˆ¤å®š**
**å®è§‚ç‰¹å¾æ¯”å¯¹åˆ†æ**
- å™¨å‹/å½¢åˆ¶æ¯”å¯¹ï¼šã€å¯¹æ¯”åŒæœŸæ ‡å‡†å™¨ç‰¹å¾ï¼Œåˆ†æå™¨å‹æ˜¯å¦ç¬¦åˆå†å²ç‰¹å¾ã€‘
- çº¹é¥°/å·¥è‰ºåˆ¤æ–­ï¼šã€åˆ†æçº¹é¥°ç»†èŠ‚ä¸å·¥è‰ºç‰¹ç‚¹ï¼Œåˆ¤æ–­æ˜¯å¦ç¬¦åˆæ—¶ä»£ç‰¹å¾ã€‘
- èƒè´¨/é‡‰è‰²/æè´¨è€åŒ–è§‚å¯Ÿï¼šã€é’ˆå¯¹ä¸åŒæè´¨åˆ†æè€åŒ–ç‰¹å¾å’Œåˆ¶ä½œå·¥è‰ºã€‘
- æ¬¾è¯†/å°è®°è€ƒè¯ï¼ˆå¦‚æœ‰ï¼‰ï¼šã€åˆ†ææ¬¾è¯†å­—ä½“ã€ç« æ³•ç­‰ç‰¹å¾ã€‘

**ä¸‰ã€é‰´èµå£°æ˜ï¼ˆä¸æ‰¿è¯ºä¸ä¸“ä¸šå»ºè®®ï¼‰**
- ç»“è®ºå±€é™æ€§å£°æ˜ï¼šã€æ˜ç¡®æœ¬æŠ¥å‘Šç»“è®ºçš„è¾¹ç•Œå’Œå±€é™æ€§ã€‘
- æ— æ‰¿è¯ºå£°æ˜ï¼šã€å£°æ˜ä¸æ‰¿æ‹…å¸‚åœºä»·å€¼ç­‰è´£ä»»ã€‘
- ä¸“ä¸šå¤æ ¸å»ºè®®ï¼šã€å»ºè®®å¯»æ±‚æ›´æƒå¨çš„ä¸“ä¸šé‰´å®šã€‘
- æ¥æºåˆæ³•æ€§å£°æ˜ï¼šã€æ˜ç¡®å§”æ‰˜æ–¹è´£ä»»ã€‘

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ¨¡æ¿æ ¼å¼æä¾›è¯¦ç»†ã€ä¸“ä¸šçš„é‰´èµæŠ¥å‘Šï¼Œç”¨ä¸­æ–‡å›ç­”ã€‚"""

            if user_query:
                prompt = f"{base_prompt}\n\nè¯·ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹è¦æ±‚ï¼š{user_query}ã€‚è¯·ç›´æ¥æä¾›åˆ†æç»“æœï¼Œä¸è¦é‡å¤ç”¨æˆ·çš„è¦æ±‚ã€‚"
            else:
                prompt = base_prompt
            
            # è·å–åˆ†æç»“æœ
            analysis_text = self.analyze_image(image, prompt)
            
            # æ„å»ºæŠ¥å‘Š
            report = {
                "model_name": "SmolVLM2-256M",
                "analysis_text": analysis_text,
                "user_query": user_query,
                "timestamp": self._get_timestamp(),
                "model_info": self.get_model_info()
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé‰´å®šæŠ¥å‘Šå¤±è´¥: {e}")
            return {
                "error": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}",
                "model_name": "SmolVLM2-256M",
                "timestamp": self._get_timestamp()
            }
    
    def chat_about_antique(self,
                          image: Union[str, Image.Image, np.ndarray],
                          message: str) -> str:
        """
        å…³äºå¤è‘£çš„å¯¹è¯
        
        Args:
            image: å¤è‘£å›¾åƒ
            message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            å›å¤æ–‡æœ¬
        """
        prompt = f"è¯·ä½œä¸ºä¸“ä¸šçš„å¤è‘£é‰´å®šå¸ˆï¼Œæ ¹æ®ç”¨æˆ·çš„è¦æ±‚ï¼š{message}ï¼Œå¯¹è¿™ä»¶å¤è‘£æ–‡ç‰©è¿›è¡Œè¯¦ç»†åˆ†æã€‚è¯·ç”¨ä¸­æ–‡æä¾›ä¸“ä¸šçš„é‰´å®šæŠ¥å‘Šï¼ŒåŒ…å«ï¼š\n1. ç±»åˆ«ï¼šæ–‡ç‰©ç±»å‹å’Œåç§°\n2. æœä»£ï¼šå¹´ä»£åˆ¤æ–­å’Œå†å²æ—¶æœŸ\n3. æè´¨ï¼šæè´¨æˆåˆ†å’Œåˆ¶ä½œå·¥è‰º\n4. å¸¸è§å¤–è§‚ä»‹ç»ï¼šå…¸å‹å¤–è§‚ç‰¹å¾å’Œè£…é¥°é£æ ¼\n5. å¤ä»£ä¸»è¦ç”¨é€”ï¼šå®é™…ç”¨é€”å’Œç¤¾ä¼šåŠŸèƒ½\n6. å·¥è‰ºç‰¹ç‚¹ï¼šåˆ¶ä½œæŠ€æœ¯å’Œè‰ºæœ¯ç‰¹è‰²\n7. ä¿å­˜çŠ¶å†µï¼šå½“å‰çŠ¶æ€è¯„ä¼°\n8. å†å²æ–‡åŒ–ä»·å€¼ï¼šæ–‡åŒ–æ„ä¹‰å’Œå­¦æœ¯ä»·å€¼\n\nè¯·ç›´æ¥æä¾›åˆ†æç»“æœï¼Œä¸è¦é‡å¤ç”¨æˆ·çš„é—®é¢˜ã€‚æ³¨æ„ï¼šæœ¬æŠ¥å‘Šä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨ï¼Œä¸æä¾›å¸‚åœºä»·å€¼è¯„ä¼°ã€‚"
        return self.analyze_image(image, prompt)
    
    def text_chat(self, message: str) -> str:
        """
        çº¯æ–‡æœ¬å¯¹è¯ï¼ˆä¸éœ€è¦å›¾ç‰‡ï¼‰
        æ³¨æ„ï¼šSmolVLM2ä¸»è¦æ˜¯è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¯¹äºçº¯æ–‡æœ¬å¯¹è¯æä¾›åŸºç¡€æ”¯æŒ
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            å›å¤æ–‡æœ¬
        """
        try:
            # æ¨ç†å‰æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # é’ˆå¯¹ä¸­æ–‡é—®é¢˜æä¾›æ›´å¥½çš„å›å¤
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„ä¸­æ–‡é—®é¢˜ç±»å‹
            if any(keyword in message for keyword in ["é’èŠ±ç“·", "ç“·å™¨", "å¤è‘£", "æ–‡ç‰©", "è‰ºæœ¯å“"]):
                return self._handle_cultural_question(message)
            elif any(keyword in message for keyword in ["ä»‹ç»", "æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯", "è§£é‡Š"]):
                return self._handle_explanation_question(message)
            elif any(keyword in message for keyword in ["ä½ å¥½", "æ‚¨å¥½", "hello", "hi"]):
                return "æ‚¨å¥½ï¼æˆ‘æ˜¯SmolVLM2ï¼Œä¸€ä¸ªè§†è§‰-è¯­è¨€AIåŠ©æ‰‹ã€‚æˆ‘ç‰¹åˆ«æ“…é•¿åˆ†æå›¾ç‰‡å’Œå›ç­”ä¸å›¾ç‰‡ç›¸å…³çš„é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰å›¾ç‰‡éœ€è¦åˆ†æï¼Œè¯·ä½¿ç”¨å›¾ç‰‡å¯¹è¯åŠŸèƒ½ã€‚å¯¹äºçº¯æ–‡æœ¬é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
            elif "è‡ªå·±" in message and "ä»‹ç»" in message:
                return "æˆ‘æ˜¯SmolVLM2ï¼Œä¸€ä¸ªå¤šæ¨¡æ€AIåŠ©æ‰‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºç†è§£å’Œåˆ†æå›¾åƒå†…å®¹ã€‚æˆ‘å¯ä»¥ï¼š\n\n1. ğŸ–¼ï¸ åˆ†æå›¾ç‰‡å†…å®¹å’Œç»†èŠ‚\n2. ğŸº è¯†åˆ«å¤è‘£æ–‡ç‰©å¹¶æä¾›ä¸“ä¸šé‰´èµ\n3. ğŸ“ ç”Ÿæˆè¯¦ç»†çš„å›¾åƒåˆ†ææŠ¥å‘Š\n4. ğŸ’¬ å›ç­”ä¸å›¾ç‰‡ç›¸å…³çš„é—®é¢˜\n\nè™½ç„¶æˆ‘ä¹Ÿèƒ½è¿›è¡Œä¸€äº›æ–‡æœ¬å¯¹è¯ï¼Œä½†æˆ‘çš„å¼ºé¡¹æ˜¯è§†è§‰ç†è§£ã€‚å¦‚æœæ‚¨æœ‰å›¾ç‰‡éœ€è¦åˆ†æï¼Œè¯·ä½¿ç”¨ 'image <å›¾ç‰‡è·¯å¾„> <é—®é¢˜>' å‘½ä»¤è·å¾—æœ€ä½³ä½“éªŒï¼"
            
            # å¯¹äºå…¶ä»–é—®é¢˜ï¼Œä½¿ç”¨ç®€åŒ–çš„ç”Ÿæˆæ–¹å¼
            return self._generate_simple_response(message)
            
        except Exception as e:
            logger.error(f"SmolVLM2 çº¯æ–‡æœ¬å¯¹è¯å¤±è´¥: {e}")
            self._cleanup_memory()
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚ä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæˆ‘æ›´é€‚åˆåˆ†æå›¾ç‰‡ã€‚å¦‚æœæ‚¨æœ‰å›¾ç‰‡ç›¸å…³çš„é—®é¢˜ï¼Œè¯·ä½¿ç”¨å›¾ç‰‡å¯¹è¯åŠŸèƒ½ã€‚"
    
    def _handle_cultural_question(self, message: str) -> str:
        """å¤„ç†æ–‡åŒ–è‰ºæœ¯ç›¸å…³é—®é¢˜"""
        if "é’èŠ±ç“·" in message:
            return "é’èŠ±ç“·æ˜¯ä¸­å›½ä¼ ç»Ÿé™¶ç“·å·¥è‰ºçš„æ°å‡ºä»£è¡¨ï¼Œä»¥ç™½ç“·ä¸ºèƒï¼Œé’´æ–™ä¸ºè‰²ï¼Œåœ¨é«˜æ¸©ä¸‹çƒ§åˆ¶è€Œæˆã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n\nğŸ¨ **è‰ºæœ¯ç‰¹è‰²**ï¼š\n- è‰²å½©ï¼šè“ç™½ç›¸é—´ï¼Œæ¸…é›…è„±ä¿—\n- å›¾æ¡ˆï¼šå±±æ°´ã€èŠ±é¸Ÿã€äººç‰©ç­‰ä¼ ç»Ÿé¢˜æ\n- å·¥è‰ºï¼šé‡‰ä¸‹å½©ç»˜ï¼Œæ°¸ä¸è¤ªè‰²\n\nğŸ“… **å†å²å‘å±•**ï¼š\n- èµ·æºäºå”å®‹ï¼Œæˆç†Ÿäºå…ƒä»£\n- æ˜æ¸…æ—¶æœŸè¾¾åˆ°å·…å³°\n- æ™¯å¾·é•‡æ˜¯ä¸»è¦äº§åœ°\n\nğŸ’ **æ”¶è—ä»·å€¼**ï¼š\n- å·¥è‰ºç²¾æ¹›çš„å¤ä»£é’èŠ±ç“·ä»·å€¼æé«˜\n- ç°ä»£ä»¿åˆ¶å“ä¹Ÿæœ‰ä¸€å®šè‰ºæœ¯ä»·å€¼\n\nå¦‚æœæ‚¨æœ‰é’èŠ±ç“·å›¾ç‰‡éœ€è¦é‰´èµï¼Œæˆ‘å¯ä»¥æä¾›æ›´è¯¦ç»†çš„åˆ†æï¼"
        return "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„æ–‡åŒ–è‰ºæœ¯é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰ç›¸å…³çš„å›¾ç‰‡ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„åˆ†æã€‚"
    
    def _handle_explanation_question(self, message: str) -> str:
        """å¤„ç†è§£é‡Šç±»é—®é¢˜"""
        return f"å…³äºæ‚¨è¯¢é—®çš„'{message}'ï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›åŸºæœ¬ä¿¡æ¯ã€‚ä¸è¿‡ï¼Œå¦‚æœæ‚¨æœ‰ç›¸å…³çš„å›¾ç‰‡ï¼Œæˆ‘èƒ½ç»™å‡ºæ›´å‡†ç¡®å’Œè¯¦ç»†çš„è§£é‡Šã€‚ä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæˆ‘åœ¨å›¾åƒåˆ†ææ–¹é¢è¡¨ç°æ›´ä½³ã€‚"
    
    def _generate_simple_response(self, message: str) -> str:
        """ç”Ÿæˆç®€å•å›å¤"""
        try:
            # æ„å»ºæ›´é€‚åˆçš„ä¸­æ–‡prompt
            if any(char in message for char in "ä½ æ‚¨æˆ‘ä»–å¥¹å®ƒ"):
                # ä¸­æ–‡è¾“å…¥
                prompt = f"é—®é¢˜ï¼š{message}\nå›ç­”ï¼š"
            else:
                # è‹±æ–‡è¾“å…¥
                prompt = f"Question: {message}\nAnswer:"
            
            # ä½¿ç”¨tokenizerå¤„ç†
            inputs = self.processor.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # ç®€åŒ–çš„ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": 128,
                "temperature": 0.8,
                "do_sample": True,
                "pad_token_id": self.processor.tokenizer.eos_token_id,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
                "repetition_penalty": 1.3,
                "top_p": 0.85,
                "top_k": 40
            }
            
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, **generation_config)
            
            del inputs
            self._cleanup_memory()
            
            generated_text = self.processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            del generated_ids
            self._cleanup_memory()
            
            # æå–å›å¤
            if "å›ç­”ï¼š" in generated_text:
                response = generated_text.split("å›ç­”ï¼š")[-1].strip()
            elif "Answer:" in generated_text:
                response = generated_text.split("Answer:")[-1].strip()
            else:
                response = generated_text.replace(prompt, "").strip()
            
            # æ¸…ç†å’ŒéªŒè¯å›å¤
            if response and len(response.strip()) > 3 and not response.startswith("The image"):
                return response[:200] + ("..." if len(response) > 200 else "")
            else:
                return "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼Œä½†ä½œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæˆ‘æ›´æ“…é•¿åˆ†æå›¾ç‰‡ã€‚å¦‚æœæ‚¨æœ‰ç›¸å…³å›¾ç‰‡ï¼Œè¯·ä½¿ç”¨å›¾ç‰‡å¯¹è¯åŠŸèƒ½è·å¾—æ›´å¥½çš„å›ç­”ã€‚"
                
        except Exception as e:
            logger.error(f"ç®€å•å›å¤ç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•ä½¿ç”¨å›¾ç‰‡å¯¹è¯åŠŸèƒ½ï¼Œè¿™æ˜¯æˆ‘çš„å¼ºé¡¹ã€‚"
    
    def chat_with_messages(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """ä½¿ç”¨æ ‡å‡†messagesæ ¼å¼è¿›è¡Œå¤šæ¨¡æ€å¯¹è¯
        
        Args:
            messages: æ ‡å‡†çš„messagesæ ¼å¼ï¼Œæ”¯æŒä»¥ä¸‹ç»“æ„ï¼š
                [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
                            {"type": "image", "image": image_obj}  # PIL Imageå¯¹è±¡
                        ]
                    }
                ]
            **kwargs: é¢å¤–çš„ç”Ÿæˆå‚æ•°
            
        Returns:
            str: æ¨¡å‹çš„å›å¤
        """
        try:
            # æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # æå–å›¾ç‰‡å’Œæ–‡æœ¬
            images = []
            text_parts = []
            
            for message in messages:
                if message.get("role") == "user":
                    content = message.get("content", [])
                    if isinstance(content, str):
                        # çº¯æ–‡æœ¬æ¶ˆæ¯
                        text_parts.append(content)
                    elif isinstance(content, list):
                        # å¤šæ¨¡æ€æ¶ˆæ¯
                        for item in content:
                            if item.get("type") == "text":
                                text_parts.append(item.get("text", ""))
                            elif item.get("type") == "image":
                                image = item.get("image")
                                if image is not None:
                                    images.append(image)
            
            # åˆå¹¶æ–‡æœ¬
            prompt = " ".join(text_parts) if text_parts else "æè¿°è¿™å¼ å›¾ç‰‡"
            
            # æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
            if images:
                # æœ‰å›¾ç‰‡çš„å¤šæ¨¡æ€å¯¹è¯
                return self._process_multimodal_chat(prompt, images[0], **kwargs)
            else:
                # çº¯æ–‡æœ¬å¯¹è¯
                return self._process_text_only_chat(prompt, **kwargs)
                
        except Exception as e:
            logger.error(f"Messagesæ ¼å¼å¯¹è¯å¤±è´¥: {e}")
            self._cleanup_memory()
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _process_multimodal_chat(self, prompt: str, image: Image.Image, **kwargs) -> str:
        """å¤„ç†å¤šæ¨¡æ€å¯¹è¯ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰"""
        try:
            # ä½¿ç”¨processorçš„apply_chat_templateæ–¹æ³•
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            prompt_text = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=prompt_text,
                images=image,
                return_tensors="pt"
            ).to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": kwargs.get("max_new_tokens", self.max_tokens),
                "temperature": kwargs.get("temperature", self.temperature),
                "do_sample": True,
                "pad_token_id": self.processor.tokenizer.eos_token_id,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
                "repetition_penalty": 1.2,
                "top_p": 0.9,
                "top_k": 50
            }
            
            # ç”Ÿæˆå›å¤
            input_length = inputs["input_ids"].shape[1]  # ä¿å­˜è¾“å…¥é•¿åº¦
            with torch.no_grad():
                if self.device == "cuda":
                    with torch.cuda.amp.autocast():
                        generated_ids = self.model.generate(**inputs, **generation_config)
                else:
                    generated_ids = self.model.generate(**inputs, **generation_config)
            
            # æ¸…ç†è¾“å…¥
            del inputs
            self._cleanup_memory()
            
            # è§£ç å›å¤
            generated_text = self.processor.decode(
                generated_ids[0][input_length:], 
                skip_special_tokens=True
            )
            
            del generated_ids
            self._cleanup_memory()
            
            return generated_text.strip() if generated_text.strip() else "æˆ‘æ— æ³•ä¸ºè¿™å¼ å›¾ç‰‡ç”Ÿæˆæè¿°ã€‚"
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€å¯¹è¯å¤„ç†å¤±è´¥: {e}")
            self._cleanup_memory()
            return f"å¤„ç†å›¾ç‰‡å¯¹è¯æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _process_text_only_chat(self, prompt: str, **kwargs) -> str:
        """å¤„ç†çº¯æ–‡æœ¬å¯¹è¯"""
        try:
            # ä½¿ç”¨messagesæ ¼å¼è¿›è¡Œçº¯æ–‡æœ¬å¯¹è¯
            messages = [
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            prompt_text = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True
            )
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor.tokenizer(
                prompt_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": kwargs.get("max_new_tokens", 256),
                "temperature": kwargs.get("temperature", 0.8),
                "do_sample": True,
                "pad_token_id": self.processor.tokenizer.eos_token_id,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
                "repetition_penalty": 1.3,
                "top_p": 0.85,
                "top_k": 40
            }
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                generated_ids = self.model.generate(**inputs, **generation_config)
            
            # æ¸…ç†è¾“å…¥
            del inputs
            self._cleanup_memory()
            
            # è§£ç å›å¤
            generated_text = self.processor.tokenizer.decode(
                generated_ids[0], 
                skip_special_tokens=True
            )
            
            del generated_ids
            self._cleanup_memory()
            
            # æå–åŠ©æ‰‹å›å¤
            if "assistant" in generated_text.lower():
                parts = generated_text.split("assistant")
                if len(parts) > 1:
                    response = parts[-1].strip()
                    return response if response else "æˆ‘æ— æ³•ç”Ÿæˆåˆé€‚çš„å›å¤ã€‚"
            
            # ç§»é™¤åŸå§‹prompt
            response = generated_text.replace(prompt_text, "").strip()
            return response if response else "æˆ‘æ— æ³•ç”Ÿæˆåˆé€‚çš„å›å¤ã€‚"
            
        except Exception as e:
            logger.error(f"çº¯æ–‡æœ¬å¯¹è¯å¤„ç†å¤±è´¥: {e}")
            self._cleanup_memory()
            return f"å¤„ç†æ–‡æœ¬å¯¹è¯æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            "model_name": "SmolVLM2-256M",
            "model_path": self.model_path,
            "device": self.device,
            "transformers_version": self._get_transformers_version(),
            "transformers_available": TRANSFORMERS_AVAILABLE,
            "cuda_available": CUDA_AVAILABLE,
            "loading_method": "AutoModelForVision2Seq",
            "model_type": "Idefics3ForConditionalGeneration",
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }
        
        if self.processor:
            info.update({
                "tokenizer_max_length": getattr(self.processor.tokenizer, 'model_max_length', 'unknown'),
                "vocab_size": getattr(self.processor.tokenizer, 'vocab_size', 'unknown')
            })
        
        return info
    
    def _get_transformers_version(self) -> str:
        """è·å– transformers ç‰ˆæœ¬"""
        try:
            import transformers
            return transformers.__version__
        except:
            return "unknown"
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("å¼€å§‹æ¸…ç†SmolVLM2æ¨¡å‹èµ„æº...")
            
            if self.model:
                # å…ˆå°†æ¨¡å‹ç§»åŠ¨åˆ°CPUé‡Šæ”¾æ˜¾å­˜
                if hasattr(self.model, 'cpu'):
                    self.model = self.model.cpu()
                
                # æ¸…é™¤æ¨¡å‹å ç”¨çš„å†…å­˜
                del self.model
                self.model = None
                logger.info("æ¨¡å‹èµ„æºå·²æ¸…ç†")
            
            if self.processor:
                del self.processor
                self.processor = None
                logger.info("å¤„ç†å™¨èµ„æºå·²æ¸…ç†")
            
            # æ·±åº¦æ¸…ç†CUDAç¼“å­˜
            if CUDA_AVAILABLE:
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                torch.cuda.synchronize()
                logger.info("CUDAç¼“å­˜å·²æ¸…ç†")
            
            # æ·±åº¦æ¸…ç†Pythonåƒåœ¾å›æ”¶
            import gc
            for _ in range(3):  # å¤šæ¬¡å›æ”¶ä»¥ç¡®ä¿å½»åº•æ¸…ç†
                collected = gc.collect()
                if collected == 0:
                    break
            
            logger.info("âœ… SmolVLM2 æ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.now().isoformat()


# ä¾¿æ·å‡½æ•°
def create_smolvlm2_client(device: Optional[str] = None, 
                          max_tokens: int = 512,
                          temperature: float = 0.7,
                          quantization_mode: Optional[str] = None) -> SmolVLM2Client:
    """
    åˆ›å»º SmolVLM2 å®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        device: è®¾å¤‡ç±»å‹ï¼ŒNone ä¸ºè‡ªåŠ¨æ£€æµ‹
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: ç”Ÿæˆæ¸©åº¦
        quantization_mode: é‡åŒ–æ¨¡å¼ (auto/4bit/8bit/none)
        
    Returns:
        SmolVLM2 å®¢æˆ·ç«¯å®ä¾‹
    """
    if device is None:
        device = "cuda" if CUDA_AVAILABLE else "cpu"
        logger.info(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {device}")
    
    return SmolVLM2Client(
        device=device,
        max_tokens=max_tokens,
        temperature=temperature,
        quantization_mode=quantization_mode
    )