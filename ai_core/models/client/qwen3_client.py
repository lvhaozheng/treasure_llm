#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3 å®¢æˆ·ç«¯å®ç°
æ”¯æŒ Qwen3 æ¨¡å‹çš„åŠ è½½ã€æ¨ç†å’Œå¯¹è¯åŠŸèƒ½
"""

import os
import sys
import torch
import json
import logging
from PIL import Image
import numpy as np
from typing import List, Dict, Any, Optional, Union

# ç‰ˆæœ¬æ£€æŸ¥
try:
    import transformers
    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor
    from transformers import BitsAndBytesConfig, TextIteratorStreamer
    from threading import Thread
except ImportError as e:
    print(f"âŒ å¯¼å…¥transformerså¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install transformers>=4.47.0")
    sys.exit(1)

# CUDAå¯ç”¨æ€§æ£€æŸ¥
CUDA_AVAILABLE = torch.cuda.is_available()

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class Qwen3Client:
    """Qwen3 æ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, device="auto", quantization="none", model_path=None):
        """
        åˆå§‹åŒ– Qwen3 å®¢æˆ·ç«¯
        
        Args:
            device: è®¾å¤‡é€‰æ‹© ("auto", "cuda", "cpu")
            quantization: é‡åŒ–æ¨¡å¼ ("none", "4bit", "8bit")
            model_path: æ¨¡å‹è·¯å¾„ (æœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å)
        """
        self.device = self._determine_device(device)
        self.quantization = quantization
        self.model_path = self._determine_model_path(model_path)
        
        # æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model = None
        self.tokenizer = None
        
        logger.info(f"åˆå§‹åŒ– Qwen3 å®¢æˆ·ç«¯: è®¾å¤‡={self.device}, é‡åŒ–={quantization}")
        
        # é…ç½®CUDAå†…å­˜ä¼˜åŒ–
        if self.device == "cuda":
            self._configure_cuda_memory()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
    
    def _determine_device(self, device):
        """ç¡®å®šä½¿ç”¨çš„è®¾å¤‡"""
        if device == "auto":
            return "cuda" if CUDA_AVAILABLE else "cpu"
        elif device == "cuda" and not CUDA_AVAILABLE:
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
            return "cpu"
        return device
    
    def _determine_model_path(self, model_path):
        """ç¡®å®šæ¨¡å‹è·¯å¾„"""
        if model_path:
            return model_path
        
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
        base_dir = os.path.dirname(__file__)  # models/client ç›®å½•
        models_dir = os.path.dirname(base_dir)  # models ç›®å½•
        
        possible_paths = [
            os.path.join(models_dir, "qwen3-0.6B"),  # æœ¬åœ° qwen3-0.6B æ¨¡å‹
            os.path.join(models_dir, "qwen3_model"),  # å¤‡ç”¨è·¯å¾„
        ]
        
        for path in possible_paths:
            if os.path.exists(path) and os.path.isdir(path):
                config_file = os.path.join(path, 'config.json')
                if os.path.exists(config_file):
                    logger.info(f"æ‰¾åˆ°æœ¬åœ° Qwen3 æ¨¡å‹: {path}")
                    return path
        
        # ä½¿ç”¨é»˜è®¤çš„HuggingFaceæ¨¡å‹
        logger.info("æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†ä» HuggingFace ä¸‹è½½")
        return "Qwen/Qwen2.5-7B-Instruct"  # é»˜è®¤ä½¿ç”¨Qwen2.5
    
    def _configure_cuda_memory(self):
        """é…ç½® CUDA æ˜¾å­˜ä¼˜åŒ–å‚æ•°"""
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        
        if CUDA_AVAILABLE and self.device == "cuda":
            try:
                torch.cuda.empty_cache()
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"CUDAæ˜¾å­˜ä¼˜åŒ–å·²å¯ç”¨: {gpu_memory_gb:.1f}GB GPU")
                
                if gpu_memory_gb < 8:
                    torch.cuda.set_per_process_memory_fraction(0.8)
                    logger.info("å¯ç”¨æ˜¾å­˜ä¼˜åŒ–æ¨¡å¼")
                    
            except Exception as e:
                logger.warning(f"CUDAæ˜¾å­˜ä¼˜åŒ–é…ç½®å¤±è´¥: {e}")
    
    def _init_model(self):
        """åˆå§‹åŒ– Qwen3 æ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½ Qwen3 æ¨¡å‹: {self.model_path}")
            
            # æ¸…ç†å†…å­˜
            self._cleanup_memory()
            
            # è·å–åŠ è½½é…ç½®
            load_kwargs = self._get_optimal_load_config()
            
            # åŠ è½½åˆ†è¯å™¨
            logger.info("æ­£åœ¨åŠ è½½ Tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            logger.info("æ­£åœ¨åŠ è½½ Model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **load_kwargs
            )
            
            # CPUæ¨¡å¼éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device == "cpu" and "device_map" not in load_kwargs:
                self.model = self.model.to(self.device)
            
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
            actual_dtype = next(self.model.parameters()).dtype if self.model else "unknown"
            logger.info(f"âœ… æˆåŠŸåŠ è½½ Qwen3 æ¨¡å‹")
            logger.info(f"ğŸ“ æ¨¡å‹ç±»å‹: Qwen3")
            logger.info(f"ğŸ“ è®¾å¤‡: {self.device}")
            logger.info(f"ğŸ“ æ•°æ®ç±»å‹: {actual_dtype}")
            logger.info(f"ğŸ“ è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")
            
            # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
            if CUDA_AVAILABLE and self.device == "cuda":
                allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
                logger.info(f"ğŸ“ GPUå†…å­˜ä½¿ç”¨: {allocated_memory:.1f}GB")
                
        except Exception as e:
            logger.error(f"Qwen3 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self._cleanup_memory()
            raise RuntimeError(f"Qwen3 æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _get_optimal_load_config(self):
        """è·å–æœ€ä¼˜çš„æ¨¡å‹åŠ è½½é…ç½®"""
        load_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.bfloat16,
        }
        
        # CPUæ¨¡å¼é…ç½®
        if not CUDA_AVAILABLE or self.device == "cpu":
            logger.info("ä½¿ç”¨CPUæ¨¡å¼åŠ è½½æ¨¡å‹")
            load_kwargs["device_map"] = "cpu"
            load_kwargs["torch_dtype"] = torch.float32
            return load_kwargs
        
        # GPUæ¨¡å¼é…ç½®
        try:
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"GPUæ˜¾å­˜: {gpu_memory_gb:.1f}GB")
            
            # æ ¹æ®é‡åŒ–æ¨¡å¼é€‰æ‹©é…ç½®
            if self.quantization == "4bit":
                if gpu_memory_gb < 4:
                    logger.warning("æ˜¾å­˜ä¸è¶³ï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
                    load_kwargs["device_map"] = "cpu"
                    load_kwargs["torch_dtype"] = torch.float32
                else:
                    logger.info("ä½¿ç”¨4bité‡åŒ–æ¨¡å¼")
                    load_kwargs["quantization_config"] = self._get_4bit_config()
                    load_kwargs["device_map"] = "auto"
                    
            elif self.quantization == "8bit":
                if gpu_memory_gb < 6:
                    logger.warning("æ˜¾å­˜ä¸è¶³ï¼Œé™çº§åˆ°4bité‡åŒ–")
                    load_kwargs["quantization_config"] = self._get_4bit_config()
                else:
                    logger.info("ä½¿ç”¨8bité‡åŒ–æ¨¡å¼")
                    load_kwargs["quantization_config"] = self._get_8bit_config()
                load_kwargs["device_map"] = "auto"
                
            else:  # æ— é‡åŒ–æ¨¡å¼
                if gpu_memory_gb < 8:
                    logger.warning("æ˜¾å­˜ä¸è¶³ï¼Œè‡ªåŠ¨å¯ç”¨4bité‡åŒ–")
                    load_kwargs["quantization_config"] = self._get_4bit_config()
                    load_kwargs["device_map"] = "auto"
                else:
                    logger.info("æ˜¾å­˜å……è¶³ï¼Œä½¿ç”¨bfloat16ç²¾åº¦")
                    load_kwargs["device_map"] = "auto"
                    
        except Exception as e:
            logger.warning(f"GPUé…ç½®æ£€æµ‹å¤±è´¥: {e}")
            load_kwargs["device_map"] = "auto"
        
        return load_kwargs
    
    def _get_4bit_config(self):
        """è·å–4bité‡åŒ–é…ç½®"""
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
    
    def _get_8bit_config(self):
        """è·å–8bité‡åŒ–é…ç½®"""
        return BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
    
    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜å’Œæ˜¾å­˜ç¼“å­˜"""
        try:
            import gc
            gc.collect()
            
            if CUDA_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
        except Exception as e:
            logger.warning(f"å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def chat_stream(self, messages, max_length=512, temperature=0.7, top_p=0.9):
        """ä½¿ç”¨æ ‡å‡†messagesæ ¼å¼è¿›è¡Œæµå¼å¯¹è¯"""
        try:
            if not self.model or not self.tokenizer:
                raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–")
            
            logger.info(f"å¤„ç†æµå¼å¯¹è¯ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œç¦ç”¨thinkingæ¨¡å¼
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False  # ç¦ç”¨thinkingæ¨¡å¼ï¼Œç›´æ¥è¿”å›æœ€ç»ˆç­”æ¡ˆ
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # åˆ›å»ºæµå¼è¾“å‡ºå™¨
            streamer = TextIteratorStreamer(
                self.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": max_length,
                "do_sample": True,
                "temperature": temperature,
                "top_p": top_p,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "streamer": streamer
            }
            
            # åœ¨çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
            def generate():
                with torch.no_grad():
                    self.model.generate(
                        **inputs,
                        **generation_config
                    )
            
            thread = Thread(target=generate)
            thread.start()
            
            # æµå¼è¾“å‡º
            for new_text in streamer:
                yield new_text
            
            thread.join()
            
        except Exception as e:
            logger.error(f"æµå¼å¯¹è¯å¤±è´¥: {e}")
            yield f"å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {type(e).__name__}: {str(e)}"
    
    def chat(self, messages, max_length=512, temperature=0.7, top_p=0.9):
        """ä½¿ç”¨æ ‡å‡†messagesæ ¼å¼è¿›è¡Œå¯¹è¯"""
        try:
            if not self.model or not self.tokenizer:
                raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–")
            
            logger.info(f"å¤„ç†å¯¹è¯ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œç¦ç”¨thinkingæ¨¡å¼
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False  # ç¦ç”¨thinkingæ¨¡å¼ï¼Œç›´æ¥è¿”å›æœ€ç»ˆç­”æ¡ˆ
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": max_length,
                "do_sample": True,
                "temperature": temperature,
                "top_p": top_p,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            # è§£ç å›å¤
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            logger.info(f"å¯¹è¯å®Œæˆï¼Œç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)}")
            return generated_text
            
        except Exception as e:
            logger.error(f"å¯¹è¯å¤±è´¥: {e}")
            raise
    
    def text_chat(self, question, max_length=512):
        """ç®€å•æ–‡æœ¬å¯¹è¯"""
        messages = [
            {"role": "user", "content": question}
        ]
        return self.chat(messages, max_length)
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            info = {
                "model_name": "Qwen3",
                "model_path": self.model_path,
                "device": self.device,
                "quantization": self.quantization,
                "transformers_version": self._get_transformers_version(),
                "cuda_available": CUDA_AVAILABLE,
                "model_loaded": self.model is not None,
                "tokenizer_loaded": self.tokenizer is not None
            }
            
            if self.model:
                info["model_dtype"] = str(next(self.model.parameters()).dtype)
                
            if self.tokenizer:
                info["vocab_size"] = self.tokenizer.vocab_size
                info["max_length"] = getattr(self.tokenizer, 'model_max_length', 'unknown')
            
            if CUDA_AVAILABLE and self.device == "cuda":
                info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.1f}GB"
            
            return info
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _get_transformers_version(self):
        """è·å–transformersç‰ˆæœ¬"""
        try:
            import transformers
            return transformers.__version__
        except:
            return "unknown"
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("æ­£åœ¨æ¸…ç†Qwen3æ¨¡å‹èµ„æº...")
            
            if hasattr(self, 'model') and self.model:
                del self.model
                self.model = None
            
            if hasattr(self, 'tokenizer') and self.tokenizer:
                del self.tokenizer
                self.tokenizer = None
            
            self._cleanup_memory()
            logger.info("Qwen3æ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")


def create_qwen3_client(device="auto", quantization="none", model_path=None):
    """åˆ›å»ºQwen3å®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°"""
    try:
        client = Qwen3Client(device=device, quantization=quantization, model_path=model_path)
        logger.info("Qwen3å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client
    except Exception as e:
        logger.error(f"Qwen3å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
        raise